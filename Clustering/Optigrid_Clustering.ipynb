{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preliminary tests showed, that Optigrid performed well on Data of November 2018. In this notebook we will explore a few other months and look for similarities and descrepancies in the results. We will use preprocessed data, where things like source stability and voltage breakdowns are indicated. Moreover, for now we will limit ourselfs to stable running sources, i.e. time periods with a low variance and a high current in the BCT25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module loading\n",
    "We use the Python modules from the ionsrcopt package that will be loaded in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data_from_csv(filename, cols_to_read, rows_to_read):\n",
    "    \"\"\" Read a csv file into a DataFrame\n",
    "\n",
    "    Parameters:\n",
    "        filename (string): Filename\n",
    "        cols_to_read (list of string): The column names to read, None if everything should be read\n",
    "        rows_to_read (list of int): The rown numbers to read, None if everything should be read\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading data from csv file \\'{}\\'\".format(filename))\n",
    "    if cols_to_read is None:\n",
    "        df = pd.read_csv(filename).fillna(method='ffill')\n",
    "    else:\n",
    "        df = pd.read_csv(filename, usecols=cols_to_read).fillna(method='ffill')\n",
    "\n",
    "    if rows_to_read is None:\n",
    "        return df\n",
    "    else:\n",
    "        return df.iloc[rows_to_read]\n",
    "\n",
    "def convert_column(df, column, type):\n",
    "    \"\"\" Converts the dtype of a column\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the column\n",
    "        column (string): The column name\n",
    "        type (string): dtype the column should be converted to\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The altered DataFrame or the old one, if it did not contain the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    if column in df.columns:\n",
    "        print(\"Converting column \\'{}\\' to \\'{}\\'\".format(column, type))\n",
    "        return df.astype({column:type})\n",
    "    else:\n",
    "        print(\"Column \\'{}\\' does not exist\".format(column))\n",
    "        return df\n",
    "\n",
    "def convert_column_types(df):\n",
    "    \"\"\" Convert all columns of a Dataframe of measurements to single precision values.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame to be altered\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Started type conversion of columns...\")\n",
    "    if 'Timestamp (UTC_TIME)' in df.columns:\n",
    "        print(\"Converting column \\'{}\\' to \\'{}\\'\".format('Timestamp (UTC_TIME)', 'datetime'))\n",
    "        df['Timestamp (UTC_TIME)'] = pd.to_datetime(df['Timestamp (UTC_TIME)']) \n",
    "    df = convert_column(df, 'IP.NSRCGEN:BIASDISCAQNV', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:GASSASAQN', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:SOURCEHTAQNI', 'float32')\n",
    "    df = convert_column(df, 'IP.SAIREM2:FORWARDPOWER', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLCEN.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLEXT.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLINJ.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT15:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT25:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITH.BCT41:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITL.BCT05:CURRENT', 'float32')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\" Clean the data of measurements, that are outliers, e.g. spikes in the extraction current.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the measurements.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Filtering data...\")\n",
    "    if 'ITF.BCT15:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT15:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITF.BCT25:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT25:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITH.BCT41:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT41:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITL.BCT05:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT05:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'IP.NSRCGEN:OVEN1AQNP' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:OVEN1AQNP'].apply(lambda x: np.nan if x < 4.5 else x)\n",
    "    if 'IP.SOLEXT.ACQUISITION:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SOLEXT.ACQUISITION:CURRENT'].apply(lambda x: np.nan if x < 1200 else x)\n",
    "    if 'IP.NSRCGEN:BIASDISCAQNV' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:BIASDISCAQNV'].apply(lambda x: np.nan if x == 0 else x)\n",
    "    if 'IP.SAIREM2:FORWARDPOWER' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SAIREM2:FORWARDPOWER'].apply(lambda x: np.nan if x < 500 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x > 2.5 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x < 0.5 else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/optigrid.py\n",
    "\"\"\" Implementation of the Optigrid Algorithm described in \"Optimal Grid-Clustering: Towards Breaking the Curse of\n",
    "Dimensionality in High-Dimensional Clustering\" by Hinneburg and Keim \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def estimate_distribution(data, cluster_indices, current_dimension, num_steps, bandwidth=0.2, percentage_of_values=1):\n",
    "    num_samples = 15000\n",
    "    sample_size = min(num_samples, len(cluster_indices))\n",
    "    sample = np.random.choice(cluster_indices, size=sample_size)\n",
    "    datapoints = np.expand_dims(data[sample][:,current_dimension], -1)\n",
    "    min_val = np.amin(datapoints)\n",
    "    max_val = np.amax(datapoints)\n",
    "    grid = np.linspace([min_val], [max_val], num_steps)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth, atol=1E-6, rtol=1E-4).fit(datapoints)\n",
    "    log_dens = kde.score_samples(grid)\n",
    "    return grid, np.exp(log_dens) * percentage_of_values\n",
    "\n",
    "def create_cuts_kde(data, cluster_indices, q, max_cut_score, noise_level, current_dimension, bandwidth=0.1, resolution=100, percentage_of_values=1):\n",
    "    grid, kde = estimate_distribution(data, cluster_indices, current_dimension, resolution, bandwidth=bandwidth, percentage_of_values=percentage_of_values) \n",
    "    \n",
    "    #plt.plot(grid, kde)\n",
    "    #plt.title(\"Current dimension: {}\".format(current_dimension))\n",
    "    #plt.show()\n",
    "    \n",
    "    kde = np.append(kde, 0)\n",
    "\n",
    "    max=[]\n",
    "    prev = 0\n",
    "    current = kde[0]\n",
    "    for bin in range(1, resolution+1): # Find all peaks that are above the noise level\n",
    "        next = kde[bin] \n",
    "        if current > prev and current > next and current >= noise_level:\n",
    "            max.append(bin-1)\n",
    "        prev = current\n",
    "        current = next\n",
    "    \n",
    "    if not max:\n",
    "        return []\n",
    "\n",
    "    max = [max[0]] + sorted(sorted(max[1:-1], key=lambda x: kde[x], reverse=True)[:q-1]) + [max[len(max)-1]] # and get the q-1 most important peaks between the leftest and rightest one.\n",
    "\n",
    "    best_cuts = [] \n",
    "    for i in range(len(max)-1): # between these peaks search for the optimal cutting plane\n",
    "        current_min = 1\n",
    "        current_min_index = -1\n",
    "        for j in range(max[i]+1, max[i+1]):\n",
    "            if kde[j] < current_min:\n",
    "                current_min = kde[j]\n",
    "                current_min_index = j\n",
    "        \n",
    "        if current_min_index >= 0 and current_min < max_cut_score:\n",
    "            best_cuts.append((grid[current_min_index], current_dimension, current_min)) # cutting plane format: (cutting coordinate, dimension in which we cut, density at minimum)\n",
    "    return best_cuts\n",
    "\n",
    "def create_cuts_histogram(data, cluster_indices, q, max_cut_score, noise_level, current_dimension, bins):\n",
    "    hist, edges = np.histogram([data[ind][current_dimension] for ind in cluster_indices], density=True, bins=bins[current_dimension]) # First create the histogram of this dimension, \n",
    "    hist = np.append(hist, 0) # adding a zero density at the end to avoid the special case in the next loop, when searching for the maxima\n",
    "    hist *= edges[1] - edges[0]\n",
    "    \n",
    "    max=[]\n",
    "    prev = 0\n",
    "    current = hist[0]\n",
    "    for bin in range(len(bins[current_dimension])): # Find all peaks that are above the noise level\n",
    "        next = hist[bin]\n",
    "        if current > prev and current > next and current >= noise_level:\n",
    "            max.append(bin-1)\n",
    "        prev = current\n",
    "        current = next\n",
    "    \n",
    "    if not max:\n",
    "        return []\n",
    "\n",
    "    max = [max[0]] + sorted(sorted(max[1:-1], key=lambda x: hist[x], reverse=True)[:q-1]) + [max[len(max)-1]] # and get the q-1 most important peaks between the leftest and rightest one.\n",
    "\n",
    "    best_cuts = [] \n",
    "    for i in range(len(max)-1): # between these peaks search for the optimal cutting plane\n",
    "        current_min = 1\n",
    "        current_min_index = -1\n",
    "        for j in range(max[i]+1, max[i+1]):\n",
    "            if hist[j] < current_min:\n",
    "                current_min = hist[j]\n",
    "                current_min_index = j\n",
    "        \n",
    "        if current_min_index >= 0 and current_min < max_cut_score:\n",
    "            best_cuts.append(((edges[current_min_index] + edges[current_min_index+1])/2, current_dimension, current_min)) # cutting plane format: (cutting coordinate, dimension in which we cut, density at minimum)\n",
    "    return best_cuts\n",
    "\n",
    "def fill_grid(data, cluster_indices, cuts):\n",
    "    \"\"\" Partitions the grid based on the selected cuts and assignes each cell the corresponding data points (as indices)\"\"\"\n",
    "    \n",
    "    num_cuts = len(cuts)\n",
    "    grid_index = np.zeros(len(cluster_indices))\n",
    "    for i, cut in enumerate(cuts):\n",
    "        cut_val = 2 ** i\n",
    "        grid_index[np.take(np.take(data, cut[1], axis=1), cluster_indices) > cut[0]] += cut_val\n",
    "\n",
    "    return [cluster_indices[grid_index==key] for key in range(2**num_cuts)]\n",
    "\n",
    "def optigrid(data, d, q, max_cut_score, noise_level, cluster_indices=np.array([]), percentage_of_values=1):\n",
    "    \"\"\" Main entry point of the algorithm. \n",
    "\n",
    "    Parameters:\n",
    "        data (list of datapoints): The whole set of datapoints to be considered\n",
    "        cluster_indices (list of int): The currently considered cluster. This is a list of indices with which the datapoints can be looked up in the data list. If None, then the whole set is considered as one cluster, typically whe the algorithm is started.\n",
    "        d (int): The number of dimensions of the data\n",
    "        q (int): number of cutting planes in each iteration\n",
    "        max_cut_score (double): The maximum density (percentage) in the density estimation histograms that will be used when creating cutting planes. The lower the more different peaks will be grouped inside one cluster.\n",
    "        noise_level (double): The background noise, everything below this threshold will not influence the cutting planes. As percentage of density.\n",
    "    \n",
    "    Returns:\n",
    "        list of list of int: Each list in this list represents a cluster. The values are again indices which that the datapoints can be looked up in the data list.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cluster_indices.size == 0:\n",
    "        cluster_indices = np.array(range(0, len(data)))\n",
    "\n",
    "    cuts = []\n",
    "    for i in range(d): # First create all best cuts\n",
    "        cuts += create_cuts_kde(data, cluster_indices, q, max_cut_score, noise_level, current_dimension=i, percentage_of_values=percentage_of_values)\n",
    "    \n",
    "    if not cuts:\n",
    "        return [cluster_indices]\n",
    "    \n",
    "    cuts = sorted(cuts, key=lambda x: x[2]) # Sort the cuts based on the density at the minima\n",
    "\n",
    "    grid = fill_grid(data, cluster_indices, cuts[:q]) # Fill the subgrid based on the cuts\n",
    "    \n",
    "    result = []\n",
    "    for cluster in grid:\n",
    "        if cluster.size==0:\n",
    "            continue\n",
    "        print(\"In current cluster: {}\".format(percentage_of_values*len(cluster)/len(cluster_indices)))\n",
    "        result += optigrid(data=data, d=d, q=q, max_cut_score=max_cut_score, noise_level=noise_level, cluster_indices=cluster, percentage_of_values=percentage_of_values*len(cluster)/len(cluster_indices)) # Run Optigrid on every subgrid\n",
    "    \n",
    "    return result\n",
    "\n",
    "def describe_cluster(cluster, columns):\n",
    "    \"\"\" Generate descriptive statistics for a cluster\n",
    "\n",
    "    Parameters:\n",
    "        cluster (DataFrame): A dataframe, that contains density informations for every bin in the cluster\n",
    "        columns (list of string): The names of the columns for which to generate statistics\n",
    "\n",
    "    Returns: \n",
    "        Series: All statistics for the selected columns\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = cluster.mean(axis = 0) \n",
    "    std = cluster.std(axis=0)\n",
    "    quantiles = cluster.quantile([.25, .5, .75], axis=0)\n",
    "    mins = cluster.min(axis=0)\n",
    "    maxs = cluster.max(axis=0)\n",
    "    \n",
    "    count = cluster.count(axis=0)[0]\n",
    "    \n",
    "    result_columns = [[mean[i], std[i], std[i] / abs(mean[i]) * 100, mins[i], quantiles.iloc[0, i], quantiles.iloc[1, i], quantiles.iloc[2, i], maxs[i]] for i in range(len(columns))]\n",
    "    result = list(itertools.chain(*result_columns)) + [count]\n",
    "    \n",
    "    value_columns = [[(col, 'mean'), (col, 'std'), (col, 'varC (%)'), (col, 'min'), (col, '25%'), (col, '50%'), (col, '75%'), (col, 'max')] for col in columns]\n",
    "    index = list(itertools.chain(*value_columns)) + [('DENSITY', 'count')]\n",
    "    \n",
    "    return pd.Series(result, index=pd.MultiIndex.from_tuples(index))\n",
    "\n",
    "\n",
    "def describe_clusters(df, columns):\n",
    "    \"\"\" Summarize all clusters and sort them by density\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): A frame containing density and cluster information about every bin\n",
    "        columns (list of string): The names of the columns for which to generate statistics\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Descriptive frame sorted by density\n",
    "    \"\"\"\n",
    "\n",
    "    result = df[columns + ['OPTIGRID_CLUSTER']].groupby('OPTIGRID_CLUSTER').apply(describe_cluster, columns)\n",
    "    return result.sort_values(('DENSITY', 'count'), ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
