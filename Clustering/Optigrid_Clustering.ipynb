{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preliminary tests showed, that Optigrid performed well on Data of November 2018. In this notebook we will explore a few other months and look for similarities and descrepancies in the results. We will use preprocessed data, where things like source stability and voltage breakdowns are indicated. Moreover, for now we will limit ourselfs to stable running sources, i.e. time periods with a low variance and a high current in the BCT25. We use the already preprocessed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module loading\n",
    "We use the Python modules from the ionsrcopt package that will be loaded in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ionsrcopt/import_notebooks/Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ionsrcopt/import_notebooks/Clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specifiy all the columns we are interested in. There are three types: Parameters, these are the ones that will be clustered later on, Measurments and columns from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [SourceFeatures.TIMESTAMP]\n",
    "parameters = [\n",
    "        SourceFeatures.BIASDISCAQNV, \n",
    "        SourceFeatures.GASAQN, \n",
    "        SourceFeatures.OVEN1AQNP,\n",
    "        SourceFeatures.SAIREM2_FORWARDPOWER,\n",
    "        SourceFeatures.SOLINJ_CURRENT,\n",
    "        SourceFeatures.SOLCEN_CURRENT,\n",
    "        SourceFeatures.SOLEXT_CURRENT,\n",
    "        SourceFeatures.SOURCEHTAQNI,\n",
    "        SourceFeatures.BCT25_CURRENT]\n",
    "measurements = []\n",
    "preprocessing = [\n",
    "        ProcessingFeatures.SOURCE_STABILITY, \n",
    "        ProcessingFeatures.HT_VOLTAGE_BREAKDOWN, \n",
    "        ProcessingFeatures.DATAPOINT_DURATION]\n",
    "\n",
    "columns_to_load = time + parameters + measurements + preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the important files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../Data_Preprocessed/'\n",
    "input_files = ['Jan2018.csv', 'Feb2018.csv', 'Mar2018.csv', 'Apr2018.csv', 'May2018.csv', 'Jun2018.csv', 'Jul2018.csv', 'Aug2018.csv', 'Sep2018.csv', 'Oct2018.csv', 'Nov2018.csv']\n",
    "input_paths = [input_folder + f for f in input_files]\n",
    "output_folder = '../Data_Clustered/'\n",
    "output_file = 'JanNov2018_lownoise.csv'\n",
    "output_path = output_folder + output_file\n",
    "\n",
    "cluster_logfile = output_folder + 'cluster_runs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv file '../Data_Preprocessed/Jan2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Feb2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Mar2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Apr2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/May2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Jun2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Jul2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Aug2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Sep2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Oct2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Nov2018.csv'\n",
      "Started type conversion of columns...\n",
      "Converting column 'IP.NSRCGEN:BIASDISCAQNV' to 'float32'\n",
      "Converting column 'IP.NSRCGEN:GASSASAQN' to 'float32'\n",
      "Converting column 'IP.NSRCGEN:OVEN1AQNP' to 'float32'\n",
      "Converting column 'IP.SOLCEN.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'IP.SOLEXT.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'ITF.BCT25:CURRENT' to 'float32'\n",
      "Converting column 'source_stable' to 'int32'\n",
      "Converting column 'is_breakdown' to 'int32'\n",
      "Converting column 'duration_seconds' to 'float32'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                            50269944\n",
       "IP.NSRCGEN:BIASDISCAQNV          25134972\n",
       "IP.NSRCGEN:GASSASAQN             25134972\n",
       "IP.SOLCEN.ACQUISITION:CURRENT    25134972\n",
       "IP.SOLEXT.ACQUISITION:CURRENT    25134972\n",
       "IP.NSRCGEN:OVEN1AQNP             25134972\n",
       "ITF.BCT25:CURRENT                25134972\n",
       "duration_seconds                 25134972\n",
       "source_stable                    25134972\n",
       "is_breakdown                     25134972\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = read_data_from_csv(input_paths, columns_to_load, None)\n",
    "df_total.dropna(inplace=True)\n",
    "df_total = convert_column_types(df_total)\n",
    "df_total.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6283743, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select what data we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_values(df_total, parameters, source_stability, voltage_breakdown):\n",
    "    data = df_total.loc[source_stability & voltage_breakdown, parameters].values\n",
    "    weights = df_total.loc[source_stability & voltage_breakdown, 'duration_seconds'].values\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready we can begin clustering. But first we standard scale it, so that all parameters have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def scale_values(values, scaler):\n",
    "    if not scaler:\n",
    "        scaler = preprocessing.StandardScaler().fit(values)\n",
    "    values_scaled = scaler.transform(values)\n",
    "    return scaler, values_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for optigrid can be chosen by visually examening the distribution of normalized data, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(parameters)\n",
    "q=1\n",
    "max_cut_score = 0.1\n",
    "noise_level = 0.05\n",
    "\n",
    "optigrid_params = {\n",
    "    'd' : d, \n",
    "    'q' : q, \n",
    "    'max_cut_score' : max_cut_score, \n",
    "    'noise_level' : noise_level,\n",
    "    'kde_bandwidth' : 0.06,\n",
    "    'verbose' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_optigrid(values_scaled, weights, optigrid_params):\n",
    "    optigrid = Optigrid(**optigrid_params)\n",
    "    optigrid.fit(values_scaled, weights)\n",
    "    return optigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the clusters are found, we set an according column in the original dataframe containing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_df_total(df_total, optigrid, num_values, source_stability, voltage_breakdown_selection):\n",
    "    clusters = np.zeros(num_values)\n",
    "\n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        clusters[cluster] = i\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, ProcessingFeatures.CLUSTER] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we bundle all these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df_total, parameters, source_stable, optigrid_params):\n",
    "    print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "    source_stability = df_total[ProcessingFeatures.SOURCE_STABILITY] == source_stable\n",
    "    voltage_breakdown_selection = df_total[ProcessingFeatures.HT_VOLTAGE_BREAKDOWN] > 0\n",
    "    \n",
    "    values, weights = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "    scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "    optigrid = run_optigrid(values_scaled, weights, optigrid_params) # and compute the clusters.\n",
    "    assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "    #print(\"Calculating cluster performance cluster performance\")\n",
    "    #cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)\n",
    "    #cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)\n",
    "    \n",
    "    print(\"Scoring voltage breakdowns\")\n",
    "    values, weights = select_values(df_total, parameters, source_stability, voltage_breakdown_selection) # Now, get the datapoints when the voltage broke down\n",
    "    _, values_scaled = scale_values(values, scaler) # scale it to the same ranges\n",
    "    scored_samples = optigrid.score_samples(values_scaled) # and find the corresponding clusters.\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, ProcessingFeatures.CLUSTER] = scored_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering for source stability 1\n",
      "Found following cuts: [(3.596400068263814, 1, 1.1390270967987154e-63)]\n",
      "Evaluating subgrid: 93.78% of datapoints\n",
      "Found following cuts: [(1.6450260552493008, 2, 2.729867768072349e-16)]\n",
      "Evaluating subgrid: 89.44% of datapoints\n",
      "Found following cuts: [(-0.4319351658676638, 3, 0.00017611017306614555)]\n",
      "Evaluating subgrid: 44.66% of datapoints\n",
      "Found following cuts: [(-0.9625741351734511, 0, 0.0006869803288679147)]\n",
      "Evaluating subgrid: 15.51% of datapoints\n",
      "Found following cuts: [(0.260421243580905, 2, 0.0008124375209109665)]\n",
      "Evaluating subgrid: 11.07% of datapoints\n",
      "Found following cuts: [(-1.8709337494590064, 0, 0.0013540321303787846)]\n",
      "Evaluating subgrid: 1.69% of datapoints\n",
      "Found cluster 0: 1.69% of datapoints\n",
      "Evaluating subgrid: 9.38% of datapoints\n",
      "Found following cuts: [(-0.28548553856936365, 4, 0.0033225242744558464)]\n",
      "Evaluating subgrid: 4.99% of datapoints\n",
      "Found following cuts: [(-1.0226569795849347, 3, 2.6882020979757727e-05)]\n",
      "Evaluating subgrid: 1.43% of datapoints\n",
      "Found cluster 1: 1.43% of datapoints\n",
      "Evaluating subgrid: 3.56% of datapoints\n",
      "Found cluster 2: 3.56% of datapoints\n",
      "Evaluating subgrid: 4.40% of datapoints\n",
      "Found following cuts: [(0.2584630268992799, 1, 0.0017349799326620684)]\n",
      "Evaluating subgrid: 1.19% of datapoints\n",
      "Found cluster 3: 1.19% of datapoints\n",
      "Evaluating subgrid: 3.21% of datapoints\n",
      "Found cluster 4: 3.21% of datapoints\n",
      "Evaluating subgrid: 4.44% of datapoints\n",
      "Found following cuts: [(-1.6168519487284652, 3, 0.018379888530038347)]\n",
      "Evaluating subgrid: 2.81% of datapoints\n",
      "Found cluster 5: 2.81% of datapoints\n",
      "Evaluating subgrid: 1.63% of datapoints\n",
      "Found cluster 6: 1.63% of datapoints\n",
      "Evaluating subgrid: 29.15% of datapoints\n",
      "Found following cuts: [(-0.5149959325790405, 0, 0.0037011985929364323)]\n",
      "Evaluating subgrid: 1.01% of datapoints\n",
      "Found cluster 7: 1.01% of datapoints\n",
      "Evaluating subgrid: 28.14% of datapoints\n",
      "Found following cuts: [(0.26042163372039795, 2, 0.004691829886478228)]\n",
      "Evaluating subgrid: 26.46% of datapoints\n",
      "Found following cuts: [(0.31414623693986365, 0, 0.009878000954336686)]\n",
      "Evaluating subgrid: 7.54% of datapoints\n",
      "Found following cuts: [(-0.5374883718863883, 2, 0.0010694234028081281)]\n",
      "Evaluating subgrid: 2.11% of datapoints\n",
      "Found cluster 8: 2.11% of datapoints\n",
      "Evaluating subgrid: 5.44% of datapoints\n",
      "Found following cuts: [(-0.08546156353420681, 0, 0.01805701530594319)]\n",
      "Evaluating subgrid: 2.72% of datapoints\n",
      "Found following cuts: [(-1.1675095811034693, 3, 0.004336221985790338)]\n",
      "Evaluating subgrid: 1.06% of datapoints\n",
      "Found cluster 9: 1.06% of datapoints\n",
      "Evaluating subgrid: 1.66% of datapoints\n",
      "Found cluster 10: 1.66% of datapoints\n",
      "Evaluating subgrid: 2.72% of datapoints\n",
      "Found following cuts: [(0.16061328938513086, 1, 0.006086296211720197)]\n",
      "Evaluating subgrid: 1.35% of datapoints\n",
      "Found cluster 11: 1.35% of datapoints\n",
      "Evaluating subgrid: 1.37% of datapoints\n",
      "Found cluster 12: 1.37% of datapoints\n",
      "Evaluating subgrid: 18.92% of datapoints\n",
      "Found following cuts: [(0.7656854041899093, 0, 0.009341441307928949)]\n",
      "Evaluating subgrid: 11.61% of datapoints\n",
      "Found following cuts: [(-0.08385426182337463, 2, 0.009995666839327559)]\n",
      "Evaluating subgrid: 9.14% of datapoints\n",
      "Found following cuts: [(-1.0457679520953782, 3, 0.011005147746776557)]\n",
      "Evaluating subgrid: 3.29% of datapoints\n",
      "Found cluster 13: 3.29% of datapoints\n",
      "Evaluating subgrid: 5.85% of datapoints\n",
      "Found following cuts: [(-0.19823534922166308, 4, 0.071401740325854)]\n",
      "Evaluating subgrid: 3.96% of datapoints\n",
      "Found cluster 14: 3.96% of datapoints\n",
      "Evaluating subgrid: 1.89% of datapoints\n",
      "Found cluster 15: 1.89% of datapoints\n",
      "Evaluating subgrid: 2.48% of datapoints\n",
      "Found cluster 16: 2.48% of datapoints\n",
      "Evaluating subgrid: 7.30% of datapoints\n",
      "Found following cuts: [(-0.11625670077222772, 2, 0.023405719450926225)]\n",
      "Evaluating subgrid: 4.36% of datapoints\n",
      "Found following cuts: [(-0.8998400490693371, 4, 0.011801462700819053)]\n",
      "Evaluating subgrid: 1.54% of datapoints\n",
      "Found cluster 17: 1.54% of datapoints\n",
      "Evaluating subgrid: 2.82% of datapoints\n",
      "Found cluster 18: 2.82% of datapoints\n",
      "Evaluating subgrid: 2.94% of datapoints\n",
      "Found cluster 19: 2.94% of datapoints\n",
      "Evaluating subgrid: 1.68% of datapoints\n",
      "Found cluster 20: 1.68% of datapoints\n",
      "Evaluating subgrid: 44.78% of datapoints\n",
      "Found following cuts: [(0.41448838783033004, 0, 0.005835963621181283)]\n",
      "Evaluating subgrid: 30.92% of datapoints\n",
      "Found following cuts: [(-0.034850438435872544, 0, 0.005391336774983169)]\n",
      "Evaluating subgrid: 23.97% of datapoints\n",
      "Found following cuts: [(-0.6078210430915909, 4, 0.008769259729118108)]\n",
      "Evaluating subgrid: 9.21% of datapoints\n",
      "Found following cuts: [(-0.48330896731578954, 0, 0.00045755983727672697)]\n",
      "Evaluating subgrid: 3.60% of datapoints\n",
      "Found following cuts: [(-0.11665807762230296, 2, 3.7449397991660667e-06)]\n",
      "Evaluating subgrid: 2.79% of datapoints\n",
      "Found following cuts: [(-0.5008501247926191, 1, 0.06431560217269378)]\n",
      "Evaluating subgrid: 1.32% of datapoints\n",
      "Found cluster 21: 1.32% of datapoints\n",
      "Evaluating subgrid: 1.48% of datapoints\n",
      "Found cluster 22: 1.48% of datapoints\n",
      "Evaluating subgrid: 0.81% of datapoints\n",
      "Found cluster 23: 0.81% of datapoints\n",
      "Evaluating subgrid: 5.61% of datapoints\n",
      "Found following cuts: [(-0.5032959118334933, 2, 0.0010314687366358565)]\n",
      "Evaluating subgrid: 2.94% of datapoints\n",
      "Found cluster 24: 2.94% of datapoints\n",
      "Evaluating subgrid: 2.67% of datapoints\n",
      "Found cluster 25: 2.67% of datapoints\n",
      "Evaluating subgrid: 14.76% of datapoints\n",
      "Found following cuts: [(-0.9304471521666557, 0, 0.01091320980683272)]\n",
      "Evaluating subgrid: 1.74% of datapoints\n",
      "Found cluster 26: 1.74% of datapoints\n",
      "Evaluating subgrid: 13.02% of datapoints\n",
      "Found following cuts: [(0.39086826083560783, 3, 0.02477299080721376)]\n",
      "Evaluating subgrid: 10.78% of datapoints\n",
      "Found following cuts: [(-0.517638239746142, 2, 0.013502500019903775)]\n",
      "Evaluating subgrid: 7.82% of datapoints\n",
      "Found following cuts: [(-0.3565611279372013, 0, 0.016115495134963187)]\n",
      "Evaluating subgrid: 1.25% of datapoints\n",
      "Found cluster 27: 1.25% of datapoints\n",
      "Evaluating subgrid: 6.57% of datapoints\n",
      "Found following cuts: [(-0.8883501562205228, 2, 0.026332442468009632)]\n",
      "Evaluating subgrid: 3.52% of datapoints\n",
      "Found cluster 28: 3.52% of datapoints\n",
      "Evaluating subgrid: 3.05% of datapoints\n",
      "Found cluster 29: 3.05% of datapoints\n",
      "Evaluating subgrid: 2.96% of datapoints\n",
      "Found following cuts: [(-0.43049736471489225, 0, 0.0019066563236366156)]\n",
      "Evaluating subgrid: 1.23% of datapoints\n",
      "Found cluster 30: 1.23% of datapoints\n",
      "Evaluating subgrid: 1.73% of datapoints\n",
      "Found cluster 31: 1.73% of datapoints\n",
      "Evaluating subgrid: 2.24% of datapoints\n",
      "Found cluster 32: 2.24% of datapoints\n",
      "Evaluating subgrid: 6.94% of datapoints\n",
      "Found following cuts: [(-0.9246046013302274, 2, 5.733675534856198e-06)]\n",
      "Evaluating subgrid: 1.70% of datapoints\n",
      "Found cluster 33: 1.70% of datapoints\n",
      "Evaluating subgrid: 5.24% of datapoints\n",
      "Found following cuts: [(0.3905909934429208, 3, 0.0015696336187959784)]\n",
      "Evaluating subgrid: 4.75% of datapoints\n",
      "Found following cuts: [(-0.13399925135602858, 1, 0.004511960129739461)]\n",
      "Evaluating subgrid: 1.67% of datapoints\n",
      "Found cluster 34: 1.67% of datapoints\n",
      "Evaluating subgrid: 3.08% of datapoints\n",
      "Found cluster 35: 3.08% of datapoints\n",
      "Evaluating subgrid: 0.49% of datapoints\n",
      "Found cluster 36: 0.49% of datapoints\n",
      "Evaluating subgrid: 13.86% of datapoints\n",
      "Found following cuts: [(0.3777568713583128, 4, 0.0013372165657327194)]\n",
      "Evaluating subgrid: 11.26% of datapoints\n",
      "Found following cuts: [(-0.1163418184627183, 2, 6.791675774987561e-05)]\n",
      "Evaluating subgrid: 10.24% of datapoints\n",
      "Found following cuts: [(0.7322380615003181, 0, 0.0006170201335513919)]\n",
      "Evaluating subgrid: 9.18% of datapoints\n",
      "Found following cuts: [(-0.9425188158497666, 2, 0.0008963740398301828)]\n",
      "Evaluating subgrid: 4.62% of datapoints\n",
      "Found cluster 37: 4.62% of datapoints\n",
      "Evaluating subgrid: 4.55% of datapoints\n",
      "Found following cuts: [(-0.5174314506126172, 2, 0.003416166725140303)]\n",
      "Evaluating subgrid: 2.85% of datapoints\n",
      "Found cluster 38: 2.85% of datapoints\n",
      "Evaluating subgrid: 1.70% of datapoints\n",
      "Found cluster 39: 1.70% of datapoints\n",
      "Evaluating subgrid: 1.06% of datapoints\n",
      "Found cluster 40: 1.06% of datapoints\n",
      "Evaluating subgrid: 1.02% of datapoints\n",
      "Found cluster 41: 1.02% of datapoints\n",
      "Evaluating subgrid: 2.60% of datapoints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found following cuts: [(0.6930218892868119, 4, 0.030227426282021963)]\n",
      "Evaluating subgrid: 1.05% of datapoints\n",
      "Found cluster 42: 1.05% of datapoints\n",
      "Evaluating subgrid: 1.55% of datapoints\n",
      "Found cluster 43: 1.55% of datapoints\n",
      "Evaluating subgrid: 4.34% of datapoints\n",
      "Found following cuts: [(-0.5048547254668342, 1, 0.0538469865632755)]\n",
      "Evaluating subgrid: 1.44% of datapoints\n",
      "Found cluster 44: 1.44% of datapoints\n",
      "Evaluating subgrid: 2.90% of datapoints\n",
      "Found cluster 45: 2.90% of datapoints\n",
      "Evaluating subgrid: 6.22% of datapoints\n",
      "Found following cuts: [(-1.695955496168498, 4, 3.18987752811992e-14)]\n",
      "Evaluating subgrid: 4.64% of datapoints\n",
      "Found following cuts: [(0.38940274715423584, 0, 0.002252607533696952)]\n",
      "Evaluating subgrid: 2.88% of datapoints\n",
      "Found following cuts: [(-1.1227397485212847, 3, 0.030139561554889165)]\n",
      "Evaluating subgrid: 1.48% of datapoints\n",
      "Found cluster 46: 1.48% of datapoints\n",
      "Evaluating subgrid: 1.40% of datapoints\n",
      "Found cluster 47: 1.40% of datapoints\n",
      "Evaluating subgrid: 1.76% of datapoints\n",
      "Found cluster 48: 1.76% of datapoints\n",
      "Evaluating subgrid: 1.58% of datapoints\n",
      "Found cluster 49: 1.58% of datapoints\n",
      "Optigrid found 50 clusters.\n",
      "Scoring voltage breakdowns\n",
      "Starting clustering for source stability 0\n",
      "Found following cuts: [(-3.182750662769934, 3, 3.065108626786207e-37)]\n",
      "Evaluating subgrid: 31.09% of datapoints\n",
      "Found following cuts: [(1.5880273086856111, 0, 4.43647479192552e-265)]\n",
      "Evaluating subgrid: 1.85% of datapoints\n",
      "Found cluster 0: 1.85% of datapoints\n",
      "Evaluating subgrid: 29.24% of datapoints\n",
      "Found following cuts: [(-2.2926388244436247, 2, 1.1469583224110388e-188)]\n",
      "Evaluating subgrid: 27.46% of datapoints\n",
      "Found following cuts: [(7.714489156549627, 1, 8.443503503376593e-06)]\n",
      "Evaluating subgrid: 9.02% of datapoints\n",
      "Found following cuts: [(-1.6746012405915693, 4, 0.0001595877339742878)]\n",
      "Evaluating subgrid: 6.99% of datapoints\n",
      "Found following cuts: [(-0.0885089960965253, 1, 0.030320288674843068)]\n",
      "Evaluating subgrid: 3.17% of datapoints\n",
      "Found cluster 1: 3.17% of datapoints\n",
      "Evaluating subgrid: 3.82% of datapoints\n",
      "Found cluster 2: 3.82% of datapoints\n",
      "Evaluating subgrid: 2.02% of datapoints\n",
      "Found cluster 3: 2.02% of datapoints\n",
      "Evaluating subgrid: 18.44% of datapoints\n",
      "Found cluster 4: 18.44% of datapoints\n",
      "Evaluating subgrid: 1.78% of datapoints\n",
      "Found cluster 5: 1.78% of datapoints\n",
      "Evaluating subgrid: 68.91% of datapoints\n",
      "Found following cuts: [(2.2295681491042627, 0, 1.358236990158251e-10)]\n",
      "Evaluating subgrid: 58.84% of datapoints\n",
      "Found following cuts: [(-1.6205367138891509, 4, 9.677780748796916e-06)]\n",
      "Evaluating subgrid: 1.46% of datapoints\n",
      "Found cluster 6: 1.46% of datapoints\n",
      "Evaluating subgrid: 57.38% of datapoints\n",
      "Found following cuts: [(1.0195273991787075, 0, 0.0005100847599698172)]\n",
      "Evaluating subgrid: 56.48% of datapoints\n",
      "Found following cuts: [(-1.516739622511045, 0, 0.016459791494874365)]\n",
      "Evaluating subgrid: 1.15% of datapoints\n",
      "Found cluster 7: 1.15% of datapoints\n",
      "Evaluating subgrid: 55.33% of datapoints\n",
      "Found following cuts: [(1.3048820013951774, 2, 0.013656624801109341)]\n",
      "Evaluating subgrid: 50.80% of datapoints\n",
      "Found following cuts: [(-1.2713549606727832, 0, 0.0241045111429378)]\n",
      "Evaluating subgrid: 0.72% of datapoints\n",
      "Found cluster 8: 0.72% of datapoints\n",
      "Evaluating subgrid: 50.08% of datapoints\n",
      "Found following cuts: [(-0.6670398001719002, 2, 0.057355292112900066)]\n",
      "Evaluating subgrid: 6.16% of datapoints\n",
      "Found following cuts: [(-0.5740075219761241, 0, 0.03982031192783466)]\n",
      "Evaluating subgrid: 4.11% of datapoints\n",
      "Found cluster 9: 4.11% of datapoints\n",
      "Evaluating subgrid: 2.05% of datapoints\n",
      "Found following cuts: [(-0.47224265276783645, 0, 0.06812421189256772)]\n",
      "Evaluating subgrid: 0.67% of datapoints\n",
      "Found cluster 10: 0.67% of datapoints\n",
      "Evaluating subgrid: 1.38% of datapoints\n",
      "Found cluster 11: 1.38% of datapoints\n",
      "Evaluating subgrid: 43.92% of datapoints\n",
      "Found following cuts: [(0.8960981585762715, 4, 0.048959096565316364)]\n",
      "Evaluating subgrid: 30.50% of datapoints\n",
      "Found following cuts: [(-0.7878544727961223, 0, 0.028425728547110566)]\n",
      "Evaluating subgrid: 4.07% of datapoints\n",
      "Found following cuts: [(0.13536286354064941, 4, 0.003749163327059394)]\n",
      "Evaluating subgrid: 1.74% of datapoints\n",
      "Found following cuts: [(-0.060199863982923096, 3, 0.05536786446483978)]\n",
      "Evaluating subgrid: 0.90% of datapoints\n",
      "Found cluster 12: 0.90% of datapoints\n",
      "Evaluating subgrid: 0.84% of datapoints\n",
      "Found cluster 13: 0.84% of datapoints\n",
      "Evaluating subgrid: 2.33% of datapoints\n",
      "Found following cuts: [(0.6613351184912402, 4, 0.023158553382782616)]\n",
      "Evaluating subgrid: 1.45% of datapoints\n",
      "Found cluster 14: 1.45% of datapoints\n",
      "Evaluating subgrid: 0.88% of datapoints\n",
      "Found cluster 15: 0.88% of datapoints\n",
      "Evaluating subgrid: 26.43% of datapoints\n",
      "Found following cuts: [(0.6354525486628217, 4, 0.044676192504237225)]\n",
      "Evaluating subgrid: 25.25% of datapoints\n",
      "Found following cuts: [(0.4142721457914873, 2, 0.05579027268755124)]\n",
      "Evaluating subgrid: 22.83% of datapoints\n",
      "Found following cuts: [(-0.3496089415116744, 0, 0.08874229551271152)]\n",
      "Evaluating subgrid: 6.08% of datapoints\n",
      "Found following cuts: [(0.06289670684120874, 4, 0.03499897534990531)]\n",
      "Evaluating subgrid: 3.23% of datapoints\n",
      "Found cluster 16: 3.23% of datapoints\n",
      "Evaluating subgrid: 2.85% of datapoints\n",
      "Found cluster 17: 2.85% of datapoints\n",
      "Evaluating subgrid: 16.75% of datapoints\n",
      "Found following cuts: [(0.29860997862286043, 4, 0.06114614904047691)]\n",
      "Evaluating subgrid: 13.35% of datapoints\n",
      "Found following cuts: [(0.24070954322814941, 2, 0.06460039528035914)]\n",
      "Evaluating subgrid: 12.07% of datapoints\n",
      "Found following cuts: [(-0.2500912095561172, 4, 0.06955246035613)]\n",
      "Evaluating subgrid: 5.68% of datapoints\n",
      "Found following cuts: [(-0.06330724557240802, 0, 0.06466194942843619)]\n",
      "Evaluating subgrid: 3.83% of datapoints\n",
      "Found following cuts: [(-0.20140713362982776, 2, 0.04449520284973632)]\n",
      "Evaluating subgrid: 2.09% of datapoints\n",
      "Found cluster 18: 2.09% of datapoints\n",
      "Evaluating subgrid: 1.74% of datapoints\n",
      "Found cluster 19: 1.74% of datapoints\n",
      "Evaluating subgrid: 1.85% of datapoints\n",
      "Found cluster 20: 1.85% of datapoints\n",
      "Evaluating subgrid: 6.38% of datapoints\n",
      "Found following cuts: [(-0.20140713362982776, 2, 0.04923788174798622)]\n",
      "Evaluating subgrid: 1.76% of datapoints\n",
      "Found cluster 21: 1.76% of datapoints\n",
      "Evaluating subgrid: 4.62% of datapoints\n",
      "Found following cuts: [(0.147725907239047, 0, 0.03529373937826303)]\n",
      "Evaluating subgrid: 3.73% of datapoints\n",
      "Found following cuts: [(0.21074238329222705, 3, 0.04211107737788899)]\n",
      "Evaluating subgrid: 1.80% of datapoints\n",
      "Found cluster 22: 1.80% of datapoints\n",
      "Evaluating subgrid: 1.93% of datapoints\n",
      "Found following cuts: [(0.006312094854586045, 2, 0.033779651793397474)]\n",
      "Evaluating subgrid: 1.03% of datapoints\n",
      "Found cluster 23: 1.03% of datapoints\n",
      "Evaluating subgrid: 0.90% of datapoints\n",
      "Found cluster 24: 0.90% of datapoints\n",
      "Evaluating subgrid: 0.89% of datapoints\n",
      "Found cluster 25: 0.89% of datapoints\n",
      "Evaluating subgrid: 1.28% of datapoints\n",
      "Found cluster 26: 1.28% of datapoints\n",
      "Evaluating subgrid: 3.40% of datapoints\n",
      "Found following cuts: [(0.22803260431145178, 3, 0.00934745788157478)]\n",
      "Evaluating subgrid: 2.68% of datapoints\n",
      "Found cluster 27: 2.68% of datapoints\n",
      "Evaluating subgrid: 0.72% of datapoints\n",
      "Found cluster 28: 0.72% of datapoints\n",
      "Evaluating subgrid: 2.42% of datapoints\n",
      "Found following cuts: [(-0.5834843088882138, 4, 0.05080819602410484)]\n",
      "Evaluating subgrid: 1.08% of datapoints\n",
      "Found cluster 29: 1.08% of datapoints\n",
      "Evaluating subgrid: 1.33% of datapoints\n",
      "Found cluster 30: 1.33% of datapoints\n",
      "Evaluating subgrid: 1.18% of datapoints\n",
      "Found cluster 31: 1.18% of datapoints\n",
      "Evaluating subgrid: 13.42% of datapoints\n",
      "Found following cuts: [(-0.9117778467409539, 0, 0.01175972410407545)]\n",
      "Evaluating subgrid: 1.63% of datapoints\n",
      "Found cluster 32: 1.63% of datapoints\n",
      "Evaluating subgrid: 11.78% of datapoints\n",
      "Found following cuts: [(0.39574392275376746, 2, 0.038466889327723915)]\n",
      "Evaluating subgrid: 9.86% of datapoints\n",
      "Found following cuts: [(-0.6016763268095073, 0, 0.0541327285899552)]\n",
      "Evaluating subgrid: 0.95% of datapoints\n",
      "Found cluster 33: 0.95% of datapoints\n",
      "Evaluating subgrid: 8.91% of datapoints\n",
      "Found following cuts: [(0.2744819455676608, 2, 0.05067975810063766)]\n",
      "Evaluating subgrid: 8.16% of datapoints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found following cuts: [(-0.37211910161105066, 0, 0.0607664818850306)]\n",
      "Evaluating subgrid: 1.64% of datapoints\n",
      "Found cluster 34: 1.64% of datapoints\n",
      "Evaluating subgrid: 6.52% of datapoints\n",
      "Found following cuts: [(1.3206476189873435, 4, 0.05489964041489316)]\n",
      "Evaluating subgrid: 2.81% of datapoints\n",
      "Found following cuts: [(0.012897724455053217, 2, 0.05671733467444948)]\n",
      "Evaluating subgrid: 1.66% of datapoints\n",
      "Found cluster 35: 1.66% of datapoints\n",
      "Evaluating subgrid: 1.15% of datapoints\n",
      "Found cluster 36: 1.15% of datapoints\n",
      "Evaluating subgrid: 3.71% of datapoints\n",
      "Found following cuts: [(-0.052403865438519104, 0, 0.05167000464609962)]\n",
      "Evaluating subgrid: 2.62% of datapoints\n",
      "Found following cuts: [(0.006457686424255371, 2, 0.0299195030105899)]\n",
      "Evaluating subgrid: 1.65% of datapoints\n",
      "Found cluster 37: 1.65% of datapoints\n",
      "Evaluating subgrid: 0.97% of datapoints\n",
      "Found cluster 38: 0.97% of datapoints\n",
      "Evaluating subgrid: 1.09% of datapoints\n",
      "Found cluster 39: 1.09% of datapoints\n",
      "Evaluating subgrid: 0.75% of datapoints\n",
      "Found cluster 40: 0.75% of datapoints\n",
      "Evaluating subgrid: 1.92% of datapoints\n",
      "Found following cuts: [(-0.11950286184296463, 3, 0.0445647817765328)]\n",
      "Evaluating subgrid: 0.47% of datapoints\n",
      "Found cluster 41: 0.47% of datapoints\n",
      "Evaluating subgrid: 1.46% of datapoints\n",
      "Found following cuts: [(-0.024844616651535034, 3, 0.07448729882232893)]\n",
      "Evaluating subgrid: 0.61% of datapoints\n",
      "Found cluster 42: 0.61% of datapoints\n",
      "Evaluating subgrid: 0.85% of datapoints\n",
      "Found cluster 43: 0.85% of datapoints\n",
      "Evaluating subgrid: 4.53% of datapoints\n",
      "Found cluster 44: 4.53% of datapoints\n",
      "Evaluating subgrid: 0.89% of datapoints\n",
      "Found cluster 45: 0.89% of datapoints\n",
      "Evaluating subgrid: 10.07% of datapoints\n",
      "Found following cuts: [(-1.684266280646276, 4, 0.000531390895719062)]\n",
      "Evaluating subgrid: 5.03% of datapoints\n",
      "Found following cuts: [(0.41967834125865533, 1, 4.0458927153533145e-05)]\n",
      "Evaluating subgrid: 4.07% of datapoints\n",
      "Found cluster 46: 4.07% of datapoints\n",
      "Evaluating subgrid: 0.96% of datapoints\n",
      "Found cluster 47: 0.96% of datapoints\n",
      "Evaluating subgrid: 5.04% of datapoints\n",
      "Found following cuts: [(0.9349457133900039, 2, 0.0007152001743330464)]\n",
      "Evaluating subgrid: 3.32% of datapoints\n",
      "Found cluster 48: 3.32% of datapoints\n",
      "Evaluating subgrid: 1.72% of datapoints\n",
      "Found cluster 49: 1.72% of datapoints\n",
      "Optigrid found 50 clusters.\n",
      "Scoring voltage breakdowns\n"
     ]
    }
   ],
   "source": [
    "df_total[ProcessingFeatures.CLUSTER] = np.nan\n",
    "cluster(df_total, parameters, 1, optigrid_params)\n",
    "cluster(df_total, parameters, 0, optigrid_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long term storage\n",
    "We will save the clustered data to a file.\n",
    "\n",
    "First, create the logging string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[04.12.2019 13:06:30] '['../Data_Preprocessed/Jan2018.csv', '../Data_Preprocessed/Feb2018.csv', '../Data_Preprocessed/Mar2018.csv', '../Data_Preprocessed/Apr2018.csv', '../Data_Preprocessed/May2018.csv', '../Data_Preprocessed/Jun2018.csv', '../Data_Preprocessed/Jul2018.csv', '../Data_Preprocessed/Aug2018.csv', '../Data_Preprocessed/Sep2018.csv', '../Data_Preprocessed/Oct2018.csv', '../Data_Preprocessed/Nov2018.csv']' cluster results saved to '../Data_Clustered/JanNov2018_lowbandwidth.csv'. Columns used: ['IP.NSRCGEN:BIASDISCAQNV', 'IP.NSRCGEN:GASSASAQN', 'IP.SOLCEN.ACQUISITION:CURRENT', 'IP.SOLEXT.ACQUISITION:CURRENT', 'IP.NSRCGEN:OVEN1AQNP', 'ITF.BCT25:CURRENT']. Parameters used: {'d': 6, 'q': 1, 'max_cut_score': 0.1, 'noise_level': 0.05, 'kde_bandwidth': 0.06, 'verbose': True}\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "logstring = \"[{}] \\'{}\\' cluster results saved to \\'{}\\'. Columns used: {}. Parameters used: {}\\n\".format(dt_string, input_paths, output_path, parameters, optigrid_params)\n",
    "with open(cluster_logfile, \"a\") as myfile:\n",
    "    myfile.write(logstring)\n",
    "\n",
    "logstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.astype({ProcessingFeatures.CLUSTER : 'int64'})\n",
    "df_total[df_total.shift(1)==df_total] = np.nan\n",
    "df_total.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(a, b):\n",
    "    if a < b:\n",
    "        return 1 - a/b\n",
    "    elif a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return b/a - 1\n",
    "    \n",
    "def cluster_performance_silhouette(df_total, values_scaled, clusters, source_stability, voltage_breakdown_selection, num_clusters):\n",
    "    mean_distances = np.array([np.array([np.sum(np.linalg.norm(values_scaled[cluster]-x, axis=1)) / len(cluster) for cluster in clusters]) for x in values_scaled])\n",
    "    optigrid_cluster = df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster']\n",
    "    selector = np.ones((len(values_scaled), num_clusters), dtype=bool)\n",
    "    selector[range(len(values)), optigrid_cluster] = False\n",
    "    print(mean_distances)\n",
    "    print(optigrid_cluster)\n",
    "    print(selector)\n",
    "    print(np.ma.masked_array(mean_distances, ~selector))\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'] = np.amin(np.ma.masked_array(mean_distances, selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'] = np.amin(np.ma.masked_array(mean_distances, ~selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'silhouette'] = np.vectorize(silhouette_coefficient)(df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'], df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs_euclid_squared_numpy(A, B):\n",
    "    sqrA = np.broadcast_to(np.sum(np.power(A, 2), 1).reshape(A.shape[0], 1), (A.shape[0], B.shape[0]))\n",
    "    sqrB = np.broadcast_to(np.sum(np.power(B, 2), 1).reshape(B.shape[0], 1), (B.shape[0], A.shape[0])).transpose()\n",
    "\n",
    "    return sqrA - 2*np.matmul(A, B.transpose()) + sqrB\n",
    "\n",
    "def cluster_performance_dbi(values_scaled, clusters, num_clusters):\n",
    "    print(\"values_scaled: {}\".format(values_scaled))\n",
    "    values_per_cluster = [np.take(values_scaled, c, axis=0) for c in clusters]\n",
    "    means = np.array([np.mean(c, axis=0) for c in values_per_cluster])\n",
    "    print(\"values_per_cluster: {}\".format(values_per_cluster[0][:10]))\n",
    "    print(\"means: {}\".format(means))\n",
    "    assigned_cluster_mean = np.zeros((len(values_scaled), len(values_scaled[0])))\n",
    "    for i, c in enumerate(clusters):\n",
    "        assigned_cluster_mean[c] = means[i]\n",
    "    print(\"assigned_cluster_mean: {}\".format(assigned_cluster_mean))\n",
    "        \n",
    "    dists_from_means = np.linalg.norm(values_scaled-assigned_cluster_mean, axis=1)\n",
    "    print(\"dists_from_means: {}\".format([dists_from_means[c] for c in clusters]))\n",
    "    s = np.array([np.sqrt(1./len(c) * np.sum(dists_from_means[c])) for c in clusters])\n",
    "    print(\"s: {}\".format(s))\n",
    "    \n",
    "    dists_between_clusters = all_pairs_euclid_squared_numpy(means, means)\n",
    "    np.fill_diagonal(dists_between_clusters, np.nan)\n",
    "    print(\"dists_between_clusters: {}\".format(dists_between_clusters))\n",
    "    \n",
    "    r = np.tile(s, (num_clusters, 1))\n",
    "    r = (r + r.T) / dists_between_clusters\n",
    "    print(\"r: {}\".format(r))\n",
    "    d = np.nanmax(r, axis=1)\n",
    "    dbi = np.mean(d)\n",
    "    print(\"Davies-Bouldin index per cluster: {}\".format(d))\n",
    "    print(\"Davies-Bouldin index total: {}\".format(dbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_clusters(optigrid, data, parameters):\n",
    "    values = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    result = pd.DataFrame(columns = pd.MultiIndex.from_tuples([(p, v) for p in parameters for v in values] + [('DENSITY', 'count'), ('DENSITY', 'percentage')]))\n",
    "    result.index.name = 'OPTIGRID_CLUSTER'\n",
    "    \n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        cluster_data = np.take(data, cluster, axis=0)\n",
    "        mean = np.mean(cluster_data, axis=0)\n",
    "        std = np.std(cluster_data, axis=0)\n",
    "        quantiles = np.quantile(cluster_data, [0, 0.25, 0.5, 0.75, 1], axis=0)\n",
    "        cluster_description = [[mean[i], std[i], quantiles[0][i], quantiles[1][i], quantiles[2][i], quantiles[3][i], quantiles[4][i]] for i in range(len(parameters))]\n",
    "        cluster_description = [item for sublist in cluster_description for item in sublist]\n",
    "        cluster_description.append(len(cluster))\n",
    "        cluster_description.append(len(cluster)/len(data)*100)\n",
    "        result.loc[i] = cluster_description\n",
    "    return result\n",
    "\n",
    "described = describe_clusters(optigrid, data, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('DENSITY', 'percentage')]\n",
    "\n",
    "num_of_clusters_to_print = 10\n",
    "described.sort_values(by=[('DENSITY', 'percentage')], ascending=False, inplace = True)\n",
    "print(\"Sum of densities of printed clusters: {:.1f}%\".format(described.head(n=num_of_clusters_to_print)[('DENSITY', 'percentage')].sum()))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the clusters we will plot the densities of the parameters. For comparability we will use explicit ranges for the x-axis per parameter. Those ranges should be chosen beforehand by an expert to validate or falsify his intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6 # number of clusters to visualize\n",
    "data = df[parameters].values # We select the unscaled data again, because by clustering we did not change any ordering and this data corresponds to the real world\n",
    "num_datapoints = len(data)\n",
    "\n",
    "resolution = 200\n",
    "bandwidth = [1, 0.01, 1, 10, 0.1, 0.001]\n",
    "num_kde_samples = 40000\n",
    "\n",
    "parameter_ranges = [[0,0] for i in range(len(parameters))]\n",
    "parameter_ranges[0] = [-300, -200] # Biasdisc x-axis\n",
    "\n",
    "parameter_ranges[1] = [5.1, 5.3] # Gas x-axis\n",
    "#parameter_ranges[2] = [0, 3] # High voltage current x-axis\n",
    "parameter_ranges[2] = [200, 300] # SolCen current x-axis\n",
    "#parameter_ranges[3] = [900, 2100] # Forwardpower x-axis\n",
    "parameter_ranges[3] = [1200, 1300] # SolExt current x-axis\n",
    "parameter_ranges[4] = [5, 20] # Oven1 power x-axis\n",
    "parameter_ranges[5] = [0, 0.05] # BCT25 current x-axis\n",
    "\n",
    "best_clusters = sorted(optigrid.clusters, key=lambda x: len(x), reverse=True)\n",
    "for i, cluster in enumerate(best_clusters[:num_clusters]):\n",
    "    median = [described.iloc[i,described.columns.get_loc((param, '50%'))] for param in parameters]\n",
    "    plot_cluster(data, cluster, parameters, parameter_ranges, resolution=resolution, median=median, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find all high voltage breakdowns that correspond to the currently considered source stability, and find out to which cluster each datapoint belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics.append(('num_of_breakdowns', ''))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics = [[(param, 'mean')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('num_of_breakdowns', '')]\n",
    "corr_described = described[wanted_statistics].corr()\n",
    "corr_described.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std'),  (param, 'min'),  (param, 'max')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist]\n",
    "df_breakdowns.groupby('is_breakdown').describe()[wanted_statistics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "size = 10\n",
    "data = np.random.uniform(0, 1, (size, 1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(np.linalg.norm(data-x, axis=1)) for x in data]) / (size - 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "values = [0, 2, 2, 2, 3, 3, 1]\n",
    "values = np.array([[x, x] for x in values])\n",
    "clusters = [[0, 6], [1, 2, 3], [4, 5]]\n",
    "\n",
    "values, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values, clusters, len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(values, [0, 1, 1, 1, 2, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_stable = 1\n",
    "print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "source_stability = df_total['source_stable'] == source_stable\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "print(values_scaled)\n",
    "optigrid = run_optigrid(values_scaled, optigrid_params) # and compute the clusters.\n",
    "print(values_scaled)\n",
    "#assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "print(\"Calculating cluster performance cluster performance\")\n",
    "#cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
