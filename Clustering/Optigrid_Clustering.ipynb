{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preliminary tests showed, that Optigrid performed well on Data of November 2018. In this notebook we will explore a few other months and look for similarities and descrepancies in the results. We will use preprocessed data, where things like source stability and voltage breakdowns are indicated. Moreover, for now we will limit ourselfs to stable running sources, i.e. time periods with a low variance and a high current in the BCT25. We use the already preprocessed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module loading\n",
    "We use the Python modules from the ionsrcopt package that will be loaded in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data_from_csv(filename, cols_to_read, rows_to_read):\n",
    "    \"\"\" Read a csv file into a DataFrame\n",
    "\n",
    "    Parameters:\n",
    "        filename (string): Filename\n",
    "        cols_to_read (list of string): The column names to read, None if everything should be read\n",
    "        rows_to_read (list of int): The rown numbers to read, None if everything should be read\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading data from csv file \\'{}\\'\".format(filename))\n",
    "    if cols_to_read is None:\n",
    "        df = pd.read_csv(filename).fillna(method='ffill')\n",
    "    else:\n",
    "        df = pd.read_csv(filename, usecols=cols_to_read).fillna(method='ffill')\n",
    "    \n",
    "    df = df.rename(columns={'Timestamp (UTC_TIME)' : 'Timestamp'})\n",
    "\n",
    "    if rows_to_read is None:\n",
    "        return df\n",
    "    else:\n",
    "        return df.iloc[rows_to_read]\n",
    "\n",
    "def convert_column(df, column, type):\n",
    "    \"\"\" Converts the dtype of a column\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the column\n",
    "        column (string): The column name\n",
    "        type (string): dtype the column should be converted to\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The altered DataFrame or the old one, if it did not contain the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    if column in df.columns:\n",
    "        print(\"Converting column \\'{}\\' to \\'{}\\'\".format(column, type))\n",
    "        return df.astype({column:type})\n",
    "    else:\n",
    "        #print(\"Column \\'{}\\' does not exist\".format(column))\n",
    "        return df\n",
    "\n",
    "def convert_column_types(df):\n",
    "    \"\"\" Convert all columns of a Dataframe of measurements to single precision values.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame to be altered\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Started type conversion of columns...\")\n",
    "    if 'Timestamp' in df.columns:\n",
    "        print(\"Converting column \\'{}\\' to \\'{}\\'\".format('Timestamp', 'datetime'))\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp']) \n",
    "        df = df.set_index('Timestamp')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:BIASDISCAQNV', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:GASSASAQN', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:SOURCEHTAQNI', 'float32')\n",
    "    df = convert_column(df, 'IP.SAIREM2:FORWARDPOWER', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLCEN.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLEXT.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLINJ.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT15:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT25:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITH.BCT41:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITL.BCT05:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'source_stable', 'int32')\n",
    "    df = convert_column(df, 'is_breakdown', 'int32')\n",
    "    df = convert_column(df, 'duration_seconds', 'float32')\n",
    "    df = convert_column(df, 'optigrid_cluster', 'int32')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\" Clean the data of measurements, that are outliers, e.g. spikes in the extraction current.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the measurements.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Filtering data...\")\n",
    "    if 'ITF.BCT15:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT15:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITF.BCT25:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT25:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITH.BCT41:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT41:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITL.BCT05:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT05:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'IP.NSRCGEN:OVEN1AQNP' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:OVEN1AQNP'].apply(lambda x: np.nan if x < 4.5 else x)\n",
    "    if 'IP.SOLEXT.ACQUISITION:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SOLEXT.ACQUISITION:CURRENT'].apply(lambda x: np.nan if x < 1200 else x)\n",
    "    if 'IP.NSRCGEN:BIASDISCAQNV' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:BIASDISCAQNV'].apply(lambda x: np.nan if x == 0 else x)\n",
    "    if 'IP.SAIREM2:FORWARDPOWER' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SAIREM2:FORWARDPOWER'].apply(lambda x: np.nan if x < 500 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x > 2.5 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x < 0.5 else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../optigrid/grid_level.py\n",
    "class GridLevel:\n",
    "    \"\"\" Optigrid creates a nested partition of the input space. This data structure is used to represent a single level of the grid. Either it represents a cluster or it is devided further into subgrids \"\"\"\n",
    "\n",
    "    def __init__(self, cutting_planes, cluster_index):\n",
    "        \"\"\" Creates a grid level.\n",
    "\n",
    "        Parameters:\n",
    "            cutting_planes (list): The planes that are used to subdivide this grid level or None if it represents a cluster\n",
    "            cluster_index (int): The index of the represented cluster or None if it can be subdivided further\n",
    "        \"\"\"\n",
    "\n",
    "        self.cutting_planes = cutting_planes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.subgrids = []\n",
    "        self.subgrid_indices = []\n",
    "\n",
    "    def add_subgrid(self, subgrid_index, subgrid):\n",
    "        \"\"\" Add a deeper level to the grid\n",
    "\n",
    "        Parameters:\n",
    "            subgrid_index (int): For every cutting plane, the subgrid can lay either right or left. This information can be used to binary encode it all at once. This is the subgrid index\n",
    "            subgrid (GridLevel): The subgrid to add\n",
    "        \"\"\"\n",
    "\n",
    "        self.subgrid_indices.append(subgrid_index)\n",
    "        self.subgrids.append(subgrid)\n",
    "\n",
    "    def get_sublevel(self, datapoint):\n",
    "        \"\"\" For a given datapoint returns the subgrid it lies in\n",
    "        \n",
    "        Parameters: \n",
    "            datapoint (ndarray): The datapoint\n",
    "        \n",
    "        Returns:\n",
    "            GridLevel: The subgrid or -1 if it belongs to no subgrid, meaning the point is an outlier.\n",
    "        \"\"\"\n",
    "\n",
    "        if datapoint is None:\n",
    "            raise ValueError(\"Datapoint must not be None.\")\n",
    "\n",
    "        grid_index = 0\n",
    "        for i, cut in enumerate(self.cutting_planes):\n",
    "            if datapoint[cut[1]] > cut[0]:\n",
    "                grid_index += 2 ** i\n",
    "\n",
    "        if not grid_index in self.subgrid_indices:\n",
    "            return -1\n",
    "\n",
    "        return self.subgrids[self.subgrid_indices.index(grid_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../optigrid/optigrid.py\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "#from grid_level import GridLevel\n",
    "\n",
    "class Optigrid:\n",
    "    \"\"\" Implementation of the Optigrid Algorithm described in \"Optimal Grid-Clustering: Towards Breaking the Curse of Dimensionality in High-Dimensional Clustering\" by Hinneburg and Keim \"\"\"\n",
    "\n",
    "    def __init__(self, d, q, max_cut_score, noise_level, kde_bandwidth = None, kde_grid_ticks=100, kde_num_samples=15000, kde_atol=1E-6, kde_rtol=1E-4, verbose=False):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "            d (int): Dimension of the data\n",
    "            q (int): Number of cutting planes per iteration\n",
    "            max_cut_score (double): Maximum density of a cutting plane\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = d\n",
    "        self.q = q\n",
    "        self.max_cut_score = max_cut_score\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "        self.root = None\n",
    "        self.clusters = None\n",
    "        self.num_clusters = -1\n",
    "\n",
    "        self.kde_bandwidth = kde_bandwidth\n",
    "        self.kde_grid_ticks = kde_grid_ticks\n",
    "        self.kde_num_samples = kde_num_samples\n",
    "        self.kde_atol = kde_atol\n",
    "        self.kde_rtol = kde_rtol\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, data, weights=None):\n",
    "        \"\"\" Find all clusters in the data. Clusters are stored as indices pointing to the passed data, i.e. if '10' is in cluster '0' means, that data[10] is in cluster 0.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "        \"\"\"\n",
    "\n",
    "        data_count = len(data)\n",
    "        cluster_indices = np.array(range(data_count))\n",
    "\n",
    "        grid, clusters = self._iteration(data=data, weights=weights, cluster_indices=cluster_indices, percentage_of_values=1, last_cluster_name = [-1])\n",
    "        self.root = grid\n",
    "        self.clusters = clusters\n",
    "        self.num_clusters = len(clusters)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Optigrid found {} clusters.\".format(self.num_clusters))\n",
    "\n",
    "    def _iteration(self, data, weights, cluster_indices, percentage_of_values, last_cluster_name):\n",
    "        \"\"\" Do one recursive step of the optigrid algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "            current_cluster (int): (passed as list to be mutable) The last cluster name that was found, -1 if none\n",
    "\n",
    "        Returns:\n",
    "            GridLevel: The gridlevel at the current step with all its depth\n",
    "            list of list of int: All clusters in the current data chunk\n",
    "        \"\"\"\n",
    "\n",
    "        cuts_iteration = []\n",
    "        for i in range(self.d): # First create all best cuts\n",
    "            cuts_iteration += self._create_cuts_kde(data, cluster_indices, current_dimension=i, percentage_of_values=percentage_of_values, weights=weights)\n",
    "        \n",
    "        if not cuts_iteration:\n",
    "            last_cluster_name[0] += 1\n",
    "            if self.verbose:\n",
    "                print(\"Found cluster {}\".format(last_cluster_name[0]))\n",
    "\n",
    "            return GridLevel(cutting_planes=None, cluster_index=last_cluster_name[0]), [cluster_indices]\n",
    "    \n",
    "        cuts_iteration = sorted(cuts_iteration, key=lambda x: x[2])[:self.q] # Sort the cuts based on the density at the minima and select the q best ones\n",
    "        grid = GridLevel(cutting_planes=cuts_iteration, cluster_index=None)\n",
    "        \n",
    "        grid_data = self._fill_grid(data, cluster_indices, cuts_iteration) # Fill the subgrid based on the cuts\n",
    "    \n",
    "        result = []\n",
    "        for i, cluster in enumerate(grid_data):\n",
    "            if cluster.size==0:\n",
    "                continue\n",
    "            if self.verbose:\n",
    "                print(\"In current cluster: {:.2f}% of datapoints\".format(percentage_of_values*len(cluster)/len(cluster_indices)*100))\n",
    "            subgrid, subresult = self._iteration(data=data, weights=weights, cluster_indices=cluster, percentage_of_values=percentage_of_values*len(cluster)/len(cluster_indices), last_cluster_name=last_cluster_name) # Run Optigrid on every subgrid\n",
    "            grid.add_subgrid(i, subgrid)\n",
    "            result += subresult\n",
    "\n",
    "        return grid, result\n",
    "\n",
    "    def _fill_grid(self, data, cluster_indices, cuts):\n",
    "        \"\"\" Partitions the grid based on the selected cuts and assignes each cell the corresponding data points (as indices).\n",
    "        \n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            cuts (list): Cutting planes in the format (position, dimension, cutting_score)\n",
    "\n",
    "        Returns:\n",
    "            list of list of int: 2**num_cuts lists of indices representing the clusters in this level\n",
    "        \"\"\"\n",
    "        \n",
    "        num_cuts = len(cuts)\n",
    "        grid_index = np.zeros(len(cluster_indices))\n",
    "        for i, cut in enumerate(cuts):\n",
    "            cut_val = 2 ** i\n",
    "            grid_index[np.take(np.take(data, cut[1], axis=1), cluster_indices) > cut[0]] += cut_val\n",
    "\n",
    "        return [cluster_indices[grid_index==key] for key in range(2**num_cuts)]\n",
    "    \n",
    "    def _create_cuts_kde(self, data, cluster_indices, current_dimension, percentage_of_values, weights):\n",
    "        \"\"\" Find the best cuts in the specified dimension by estimating the data density using kde.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            current_dimension (int): Dimension on which to project\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "\n",
    "        Returns:\n",
    "            list: q best cuts in the format (position, dimension, cutting_score)\n",
    "        \"\"\"\n",
    "\n",
    "        grid, kde = self._estimate_distribution(data, cluster_indices, current_dimension, percentage_of_values=percentage_of_values, weights=weights) \n",
    "        kde = np.append(kde, 0)\n",
    "\n",
    "        peaks = self._find_peaks_distribution(kde)      \n",
    "        if not peaks:\n",
    "            return []\n",
    "\n",
    "        peaks = [peaks[0]] + sorted(sorted(peaks[1:-1], key=lambda x: kde[x], reverse=True)[:self.q - 1]) + [peaks[len(peaks) - 1]] # and get the q-1 most important peaks between the leftest and rightest one.\n",
    "        best_cuts = self._find_best_cuts(grid, kde, peaks, current_dimension)\n",
    "        return best_cuts\n",
    "\n",
    "    def _find_best_cuts(self, grid, kde, peaks, current_dimension):\n",
    "        \"\"\" Using a density estimate and its maxima, finds the best cutting planes\n",
    "        \n",
    "        Parameters:\n",
    "            grid (list of double): The grid on which the density estimate was evaluated\n",
    "            kde (list of double): For each point on the grid the corresponding density\n",
    "            peaks (list of double): The maxima of the density estimate on the grid\n",
    "            current_dimension (int): Dimension on which the data is projected\n",
    "\n",
    "        Returns:\n",
    "            list: Best cutting planes in this dimension in the format (position, dimension, cutting_score)\n",
    "        \"\"\"\n",
    "        best_cuts = [] \n",
    "        for i in range(len(peaks)-1): # between these peaks search for the optimal cutting plane\n",
    "            current_min = 1\n",
    "            current_min_index = -1\n",
    "            for j in range(peaks[i]+1, peaks[i+1]):\n",
    "                if kde[j] < current_min:\n",
    "                    current_min = kde[j]\n",
    "                    current_min_index = j\n",
    "            \n",
    "            if current_min_index >= 0 and current_min < self.max_cut_score:\n",
    "                best_cuts.append((grid[current_min_index], current_dimension, current_min)) # cutting plane format: (cutting coordinate, dimension in which we cut, density at minimum)\n",
    "        return best_cuts\n",
    "\n",
    "    def _find_peaks_distribution(self, kde):\n",
    "        \"\"\" Given a density distribution, locates its peaks\n",
    "\n",
    "        Parameters:\n",
    "            kde (list of double): The density estimates on an arbitrary 1D grid\n",
    "\n",
    "        Returns:\n",
    "            list of int: The corresponding indices of the grid where the kde has its peaks.\n",
    "        \"\"\"\n",
    "\n",
    "        peaks=[]\n",
    "        prev = 0\n",
    "        current = kde[0]\n",
    "        for bin in range(1, len(kde)): # Find all peaks that are above the noise level\n",
    "            next = kde[bin] \n",
    "            if current > prev and current > next and current >= self.noise_level:\n",
    "                peaks.append(bin-1)\n",
    "            prev = current\n",
    "            current = next\n",
    "        return peaks\n",
    "\n",
    "    def _estimate_distribution(self, data, cluster_indices, current_dimension, percentage_of_values, weights):\n",
    "        \"\"\" Estimate the distribution using a sample of the data projected to a coordinate axis using scikits kde estimate method\n",
    "\n",
    "        Parametes:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            current_dimension (int): Dimension on which to project\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "\n",
    "        Returns:\n",
    "            list of double: A equally spaced grid\n",
    "            list of double: The density on the grid points\n",
    "        \"\"\"\n",
    "\n",
    "        sample_size = min(self.kde_num_samples, len(cluster_indices))\n",
    "        sample = np.random.choice(cluster_indices, size=sample_size)\n",
    "        datapoints = data[sample][:,current_dimension]\n",
    "        #datapoints = np.expand_dims(datapoints, -1)\n",
    "        weights_sample = None\n",
    "        if not weights is None:\n",
    "            weights_sample = weights[sample]\n",
    "        min_val = np.amin(datapoints)\n",
    "        max_val = np.amax(datapoints)\n",
    "\n",
    "        #kde = KernelDensity(kernel=self.kde_kernel, bandwidth=self.kde_bandwidth, atol=self.kde_atol, rtol=self.kde_rtol).fit(datapoints)\n",
    "        std = datapoints.std(ddof=1)\n",
    "        if np.isclose(std, 0):\n",
    "            return 0, np.infty\n",
    "\n",
    "        kde = gaussian_kde(dataset=datapoints, bw_method=self.kde_bandwidth / std, weights=weights_sample)\n",
    "\n",
    "        grid = np.linspace(min_val, max_val, self.kde_grid_ticks)\n",
    "        #log_dens = kde.score_samples(grid)\n",
    "        dens = kde.evaluate(grid)\n",
    "        #return grid, np.exp(log_dens) * percentage_of_values\n",
    "        return grid, dens * percentage_of_values\n",
    "\n",
    "    def score_samples(self, samples):\n",
    "        \"\"\" For every sample calculates the cluster it belongs to\n",
    "\n",
    "        Parameters:\n",
    "            samples (list of ndarray): The sample to score. They need to have the same dimensionality and scale as the data optigrid was fitted with\n",
    "        \n",
    "        Returns:\n",
    "            list of int: For every sample, the cluster it belongs to or None if it is in no cluster (only possible for q>1)\n",
    "        \"\"\"\n",
    "\n",
    "        return [self._score_sample(sample) for sample in samples]\n",
    "\n",
    "    def _score_sample(self, sample):\n",
    "        \"\"\" Score a single sample\n",
    "\n",
    "        Parameters:\n",
    "            sample (ndarray): Needs to have the same dimensionality and scale as the data optigrid was fitted with\n",
    "\n",
    "        Returns:\n",
    "            int: Cluster the sample belongs to ore None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Optigrid needs to be fitted to a dataset first.\")\n",
    "\n",
    "        current_grid_level = self.root\n",
    "        while current_grid_level.cluster_index is None:\n",
    "            sub_level = current_grid_level.get_sublevel(sample)\n",
    "            if sub_level is None:\n",
    "                return None\n",
    "            \n",
    "            current_grid_level = sub_level\n",
    "\n",
    "        return current_grid_level.cluster_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specifiy all the columns we are interested in. There are three types: Parameters, these are the ones that will be clustered later on, Measurments and columns from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['Timestamp']\n",
    "parameters = ['IP.NSRCGEN:BIASDISCAQNV', 'IP.NSRCGEN:GASSASAQN', 'IP.SOLCEN.ACQUISITION:CURRENT', 'IP.SOLEXT.ACQUISITION:CURRENT', 'IP.NSRCGEN:OVEN1AQNP', 'ITF.BCT25:CURRENT']\n",
    "measurments = ['ITF.BCT25:CURRENT']\n",
    "preprocessing = ['source_stable', 'is_breakdown', 'duration_seconds']\n",
    "columns_to_load = time + parameters + measurments + preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the important files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../Data_Preprocessed/'\n",
    "input_file = 'Nov2018.csv'\n",
    "input_path = input_folder + input_file\n",
    "output_folder = '../Data_Clustered/'\n",
    "output_file = input_file\n",
    "output_path = output_folder + input_file\n",
    "\n",
    "cluster_logfile = output_folder + 'cluster_runs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv file '../Data_Preprocessed/Nov2018.csv'\n",
      "Started type conversion of columns...\n",
      "Converting column 'Timestamp' to 'datetime'\n",
      "Converting column 'IP.NSRCGEN:BIASDISCAQNV' to 'float32'\n",
      "Converting column 'IP.NSRCGEN:GASSASAQN' to 'float32'\n",
      "Converting column 'IP.SOLCEN.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'IP.SOLEXT.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'ITF.BCT25:CURRENT' to 'float32'\n",
      "Converting column 'source_stable' to 'int32'\n",
      "Converting column 'is_breakdown' to 'int32'\n",
      "Converting column 'duration_seconds' to 'float32'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                            22930784\n",
       "IP.NSRCGEN:BIASDISCAQNV          11465392\n",
       "IP.NSRCGEN:GASSASAQN             11465392\n",
       "IP.NSRCGEN:OVEN1AQNP             22930784\n",
       "IP.SOLCEN.ACQUISITION:CURRENT    11465392\n",
       "IP.SOLEXT.ACQUISITION:CURRENT    11465392\n",
       "ITF.BCT25:CURRENT                11465392\n",
       "duration_seconds                 11465392\n",
       "source_stable                    11465392\n",
       "is_breakdown                     11465392\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = read_data_from_csv(input_path, columns_to_load, None)\n",
    "df_total.dropna(inplace=True)\n",
    "df_total = convert_column_types(df_total)\n",
    "df_total.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select what data we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_values(df_total, parameters, source_stability, voltage_breakdown):\n",
    "    data = df_total.loc[source_stability & voltage_breakdown, parameters].values\n",
    "    weights = df_total.loc[source_stability & voltage_breakdown, 'duration_seconds'].values\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready we can begin clustering. But first we standard scale it, so that all parameters have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def scale_values(values, scaler):\n",
    "    if not scaler:\n",
    "        scaler = preprocessing.StandardScaler().fit(values)\n",
    "    values_scaled = scaler.transform(values)\n",
    "    return scaler, values_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for optigrid can be chosen by visually examening the distribution of normalized data, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(parameters)\n",
    "q=1\n",
    "max_cut_score = 0.3\n",
    "noise_level = 0.1\n",
    "\n",
    "optigrid_params = {\n",
    "    'd' : d, \n",
    "    'q' : q, \n",
    "    'max_cut_score' : max_cut_score, \n",
    "    'noise_level' : noise_level,\n",
    "    'kde_bandwidth' : 0.1,\n",
    "    'verbose' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_optigrid(values_scaled, weights, optigrid_params):\n",
    "    optigrid = Optigrid(**optigrid_params)\n",
    "    optigrid.fit(values_scaled, weights)\n",
    "    return optigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the clusters are found, we set an according column in the original dataframe containing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_df_total(df_total, optigrid, num_values, source_stability, voltage_breakdown_selection):\n",
    "    clusters = np.zeros(num_values)\n",
    "\n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        clusters[cluster] = i\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we bundle all these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df_total, parameters, source_stable, optigrid_params):\n",
    "    print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "    source_stability = df_total['source_stable'] == source_stable\n",
    "    voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "    \n",
    "    values, weights = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "    scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "    optigrid = run_optigrid(values_scaled, weights, optigrid_params) # and compute the clusters.\n",
    "    assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "    #print(\"Calculating cluster performance cluster performance\")\n",
    "    #cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)\n",
    "    #cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)\n",
    "    \n",
    "    print(\"Scoring voltage breakdowns\")\n",
    "    values, weights = select_values(df_total, parameters, source_stability, voltage_breakdown_selection) # Now, get the datapoints when the voltage broke down\n",
    "    _, values_scaled = scale_values(values, scaler) # scale it to the same ranges\n",
    "    scored_samples = optigrid.score_samples(values_scaled) # and find the corresponding clusters.\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster'] = scored_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering for source stability 1\n",
      "In current cluster: 75.97% of datapoints\n",
      "In current cluster: 5.87% of datapoints\n",
      "In current cluster: 2.89% of datapoints\n",
      "Found cluster 0\n",
      "In current cluster: 2.98% of datapoints\n",
      "Found cluster 1\n",
      "In current cluster: 70.10% of datapoints\n",
      "In current cluster: 29.83% of datapoints\n",
      "In current cluster: 26.24% of datapoints\n",
      "In current cluster: 6.25% of datapoints\n",
      "In current cluster: 3.08% of datapoints\n",
      "Found cluster 2\n",
      "In current cluster: 3.16% of datapoints\n",
      "Found cluster 3\n",
      "In current cluster: 19.99% of datapoints\n",
      "In current cluster: 5.02% of datapoints\n",
      "Found cluster 4\n",
      "In current cluster: 14.97% of datapoints\n",
      "In current cluster: 11.77% of datapoints\n",
      "Found cluster 5\n",
      "In current cluster: 3.20% of datapoints\n",
      "Found cluster 6\n",
      "In current cluster: 3.59% of datapoints\n",
      "Found cluster 7\n",
      "In current cluster: 40.27% of datapoints\n",
      "In current cluster: 18.90% of datapoints\n",
      "In current cluster: 8.32% of datapoints\n",
      "In current cluster: 4.54% of datapoints\n",
      "Found cluster 8\n",
      "In current cluster: 3.79% of datapoints\n",
      "Found cluster 9\n",
      "In current cluster: 10.58% of datapoints\n",
      "Found cluster 10\n",
      "In current cluster: 21.37% of datapoints\n",
      "In current cluster: 16.58% of datapoints\n",
      "In current cluster: 8.04% of datapoints\n",
      "Found cluster 11\n",
      "In current cluster: 8.53% of datapoints\n",
      "Found cluster 12\n",
      "In current cluster: 4.79% of datapoints\n",
      "Found cluster 13\n",
      "In current cluster: 24.03% of datapoints\n",
      "In current cluster: 16.44% of datapoints\n",
      "In current cluster: 2.61% of datapoints\n",
      "Found cluster 14\n",
      "In current cluster: 13.83% of datapoints\n",
      "Found cluster 15\n",
      "In current cluster: 7.59% of datapoints\n",
      "Found cluster 16\n",
      "Optigrid found 17 clusters.\n",
      "Scoring voltage breakdowns\n",
      "Starting clustering for source stability 0\n",
      "In current cluster: 89.03% of datapoints\n",
      "In current cluster: 3.30% of datapoints\n",
      "Found cluster 0\n",
      "In current cluster: 85.72% of datapoints\n",
      "In current cluster: 71.65% of datapoints\n",
      "In current cluster: 43.00% of datapoints\n",
      "In current cluster: 17.55% of datapoints\n",
      "In current cluster: 5.64% of datapoints\n",
      "Found cluster 1\n",
      "In current cluster: 11.91% of datapoints\n",
      "In current cluster: 2.94% of datapoints\n",
      "Found cluster 2\n",
      "In current cluster: 8.97% of datapoints\n",
      "Found cluster 3\n",
      "In current cluster: 25.45% of datapoints\n",
      "In current cluster: 2.84% of datapoints\n",
      "Found cluster 4\n",
      "In current cluster: 22.61% of datapoints\n",
      "Found cluster 5\n",
      "In current cluster: 28.65% of datapoints\n",
      "In current cluster: 9.01% of datapoints\n",
      "Found cluster 6\n",
      "In current cluster: 19.64% of datapoints\n",
      "In current cluster: 3.26% of datapoints\n",
      "Found cluster 7\n",
      "In current cluster: 16.39% of datapoints\n",
      "Found cluster 8\n",
      "In current cluster: 14.07% of datapoints\n",
      "Found cluster 9\n",
      "In current cluster: 10.97% of datapoints\n",
      "Found cluster 10\n",
      "Optigrid found 11 clusters.\n",
      "Scoring voltage breakdowns\n"
     ]
    }
   ],
   "source": [
    "df_total['optigrid_cluster'] = np.nan\n",
    "cluster(df_total, parameters, 1, optigrid_params)\n",
    "cluster(df_total, parameters, 0, optigrid_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long term storage\n",
    "We will save the clustered data to a file.\n",
    "\n",
    "First, create the logging string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[29.11.2019 09:29:23] '../Data_Preprocessed/Nov2018.csv' cluster results saved to '../Data_Clustered/Nov2018.csv'. Columns used: ['IP.NSRCGEN:BIASDISCAQNV', 'IP.NSRCGEN:GASSASAQN', 'IP.SOLCEN.ACQUISITION:CURRENT', 'IP.SOLEXT.ACQUISITION:CURRENT', 'IP.NSRCGEN:OVEN1AQNP', 'ITF.BCT25:CURRENT']. Parameters used: {'d': 6, 'q': 1, 'max_cut_score': 0.3, 'noise_level': 0.1, 'kde_bandwidth': 0.1, 'verbose': True}\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "logstring = \"[{}] \\'{}\\' cluster results saved to \\'{}\\'. Columns used: {}. Parameters used: {}\\n\".format(dt_string, input_path, output_path, parameters, optigrid_params)\n",
    "with open(cluster_logfile, \"a\") as myfile:\n",
    "    myfile.write(logstring)\n",
    "\n",
    "logstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.astype({'optigrid_cluster' : 'int64'})\n",
    "df_total[df_total.shift(1)==df_total] = np.nan\n",
    "df_total.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "The selection of the parameters for optigrid can be done by looking of the densities of the normalized parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_stability = df_total['source_stable'] == 1\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection)\n",
    "_, values_scaled = scale_values(values, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (50,10)\n",
    "\n",
    "def estimate_distribution(data, cluster_indices, current_dimension, num_steps, bandwidth = 0.1, percentage_of_values=1, num_kde_samples=15000):\n",
    "    sample_size = min(num_kde_samples, len(cluster_indices))\n",
    "    sample = np.random.choice(cluster_indices, size=sample_size)\n",
    "    datapoints = np.expand_dims(data[sample][:,current_dimension], -1)\n",
    "    min_val = np.amin(datapoints)\n",
    "    max_val = np.amax(datapoints)\n",
    "    grid = np.linspace([min_val], [max_val], num_steps)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth, atol=1E-6, rtol=1E-4).fit(datapoints)\n",
    "    log_dens = kde.score_samples(grid)\n",
    "    return grid, np.exp(log_dens) * percentage_of_values\n",
    "\n",
    "def plot_cluster(data, cluster_indices, parameters, parameter_ranges, median, resolution, bandwidth, percentage_of_values, num_kde_samples):\n",
    "    if isinstance(bandwidth, float):\n",
    "        bandwidth = [bandwidth for i in range(len(parameters))]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.title('Densities of specified parameters')\n",
    "    \n",
    "    for i, parameter in enumerate(parameters):\n",
    "        grid, kde = estimate_distribution(data, cluster_indices, i, resolution, bandwidth=bandwidth[i], percentage_of_values=percentage_of_values, num_kde_samples=num_kde_samples)\n",
    "        ax = plt.subplot('1{}{}'.format(len(parameters), i+1))\n",
    "        ax.set_title(\"{}\".format(parameter), fontsize=22)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "        if parameter_ranges:\n",
    "            ax.set_xlim(*parameter_ranges[i])\n",
    "            #ax.set_ylim(*parameter_ranges[i][1])\n",
    "            \n",
    "        if median is not None:\n",
    "            ax.axvline(x=median[i], color='red')\n",
    "        \n",
    "        ax.plot(grid, kde)\n",
    "    \n",
    "    plt.suptitle('Densities of specified parameters', fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_indices = np.array(range(0, len(values_scaled)))\n",
    "resolution = 200\n",
    "bandwidth = 0.2\n",
    "\n",
    "num_kde_samples = 40000\n",
    "\n",
    "plot_cluster(values_scaled, cluster_indices, parameters, parameter_ranges=None, median=None, resolution=resolution, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these graphs we chose the noise level to be 0.1 and the max_cut_score, i.e. the maximum density where we still do a cut between two peaks, as 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values_scaled: [[ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  1.00161365]\n",
      " ...\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -0.74528563]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]]\n",
      "values_per_cluster: [[-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624  0.38292068]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624  0.38292068]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624  0.38292068]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624  0.38292068]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624  0.38292068]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624 -0.1993793 ]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624 -0.1993793 ]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624 -0.1993793 ]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624 -0.1993793 ]\n",
      " [-1.20214145  5.61453901 -0.87421725  0.1507204  -1.50693624 -0.1993793 ]]\n",
      "means: [[-1.39035601  1.25192659 -0.87439849  0.06283379 -1.45165122 -0.63836176]\n",
      " [-1.41126894  0.57431276 -0.14207584  0.06134715 -1.3794607  -0.18102137]\n",
      " [-0.34929942 -1.47417313 -0.8741356   0.05834592 -1.39649968 -0.07587427]\n",
      " [-0.39864234 -0.85757847 -0.14205675  0.05561457 -1.2955035  -0.38585499]\n",
      " [ 0.10359374  2.27482799  0.0494518   0.05844646 -1.37075673 -0.7000636 ]\n",
      " [ 1.18399217 -0.99714367 -0.14205675  0.05286999 -1.1655868  -1.16821435]\n",
      " [-0.96880269  0.47273245 -0.87379542 -1.00116125  0.03429471 -0.61593805]\n",
      " [-1.40830904 -0.36991962 -0.65871303  0.0598186   0.23643921 -0.06652556]\n",
      " [-2.3750459  -0.53530664 -0.1409328   0.05838289 -0.06807916 -0.86896809]\n",
      " [-0.41811144  0.05580366 -0.86057762 -0.00323718  0.09181227 -0.05721324]\n",
      " [ 0.83555676 -0.04849603 -0.69141382 -0.4669685   0.21383955  0.18510813]\n",
      " [ 0.62765416 -0.06546206 -0.87311916  0.06229226  0.33724377  0.42684539]\n",
      " [ 0.63820544  0.13633953 -0.80143734  0.0583526   0.65182806  0.62277882]\n",
      " [ 1.95153564 -0.66174954 -0.6921984   0.05876827  0.54672486  0.49012914]\n",
      " [ 1.6543222   0.68029652 -0.13058477  0.06860489 -0.02784755 -0.40030493]\n",
      " [ 0.85449717  0.39161065  1.68843105  0.06041337  0.94388672  0.58574841]\n",
      " [ 0.66859878  0.22425738  1.68849054  0.05650197  1.34583545  0.49567437]]\n",
      "assigned_cluster_mean: [[ 1.6543222   0.68029652 -0.13058477  0.06860489 -0.02784755 -0.40030493]\n",
      " [ 1.6543222   0.68029652 -0.13058477  0.06860489 -0.02784755 -0.40030493]\n",
      " [ 1.6543222   0.68029652 -0.13058477  0.06860489 -0.02784755 -0.40030493]\n",
      " ...\n",
      " [-0.34929942 -1.47417313 -0.8741356   0.05834592 -1.39649968 -0.07587427]\n",
      " [-0.34929942 -1.47417313 -0.8741356   0.05834592 -1.39649968 -0.07587427]\n",
      " [-0.34929942 -1.47417313 -0.8741356   0.05834592 -1.39649968 -0.07587427]]\n",
      "dists_from_means: [array([4.48571178, 4.48571178, 4.48571178, ..., 2.0437053 , 2.0437053 ,\n",
      "       2.0437053 ]), array([0.98797054, 0.98797054, 0.98797054, ..., 4.97721156, 4.97721156,\n",
      "       4.97721156]), array([ 2.64269966,  2.64269966, 18.25338363, ...,  0.81443779,\n",
      "        1.57627413,  1.57627413]), array([0.77284662, 0.77284662, 0.77284662, ..., 0.46076945, 0.46076945,\n",
      "       0.46076945]), array([5.55213843, 5.55213843, 5.55213843, ..., 3.73598558, 3.73598558,\n",
      "       3.73598558]), array([0.17854717, 0.17854717, 0.17854717, ..., 0.19807018, 0.19807018,\n",
      "       8.82186166]), array([1.7824085, 1.7824085, 1.7824085, ..., 1.7401514, 1.7401514,\n",
      "       1.7401514]), array([1.19450168, 1.19450168, 1.19450168, ..., 1.94217031, 1.94217031,\n",
      "       1.94217031]), array([1.23199804, 1.23199804, 1.23199804, ..., 0.47066891, 0.47066891,\n",
      "       0.47066891]), array([10.01308426, 10.01308426, 10.01308426, ...,  1.6383687 ,\n",
      "        1.6383687 ,  1.6383687 ]), array([0.2679272 , 0.2679272 , 0.2679272 , ..., 1.08051287, 1.08051287,\n",
      "       1.08051287]), array([1.66104536, 1.66104536, 1.66104536, ..., 0.90957452, 0.90957452,\n",
      "       0.90957452]), array([1.27738543, 1.27738543, 1.27738543, ..., 1.84802708, 1.84802708,\n",
      "       1.84802708]), array([5.06551781, 5.06551781, 5.06551781, ..., 1.0834272 , 1.0834272 ,\n",
      "       1.0834272 ]), array([1.02974404, 1.02974404, 1.42217116, ..., 2.09563939, 2.09563939,\n",
      "       2.09563939]), array([1.7808946 , 1.7808946 , 1.7808946 , ..., 0.28561577, 0.28561577,\n",
      "       0.28561577]), array([0.45984067, 0.45984067, 0.45984067, ..., 7.93020309, 7.93020309,\n",
      "       7.93020309])]\n",
      "s: [1.24035651 0.77705183 0.70041017 0.67838116 1.79361773 0.85827979\n",
      " 0.75667442 0.87817395 0.91793335 0.90473119 0.78326015 0.79754106\n",
      " 0.83948618 0.85322246 0.96908765 0.80961249 0.97188495]\n",
      "dists_between_clusters: [[        nan  1.21026823  8.83487262  6.05802592  4.14208279 12.58458597\n",
      "   4.12547462  5.85388266  6.66925945  5.10053909 10.41192267 10.14269143\n",
      "  11.38004013 20.13060821 12.23400644 19.58465814 20.9758087 ]\n",
      " [ 1.21026823         nan  5.87134043  3.12476358  5.49273287 10.22521944\n",
      "   4.05828945  3.78274165  4.35312356  3.95560204  8.68966264  8.41752073\n",
      "   9.59916114 17.29957995 11.2842097  14.50371161 15.68454149]\n",
      " [ 8.83487262  5.87134043         nan  1.02485888 15.50341036  4.36103723\n",
      "   7.63562266  5.05386305  7.91640431  4.56496106  6.40708607  6.1975152\n",
      "   8.25796457 10.08345683 11.18767605 17.41218045 18.33490882]\n",
      " [ 6.05802592  3.12476358  1.02485888         nan 10.20528496  3.15318251\n",
      "   5.56832807  3.97300912  5.75000786  3.38703217  5.35686338  5.54157533\n",
      "   7.30715894 10.02550481  8.18718365 12.44042238 13.414042  ]\n",
      " [ 4.14208279  5.49273287 15.50341036 10.20528496         nan 12.17102581\n",
      "   8.35398301 12.76648844 15.80225501  8.58056833 10.05302145 10.78993841\n",
      "  11.42371622 17.68171637  6.87306601 13.80750718 16.02018465]\n",
      " [12.58458597 10.22521944  4.36103723  3.15318251 12.17102581         nan\n",
      "  10.18620962 10.55981269 14.17414845  7.0102302   5.32766361  6.51479894\n",
      "   8.52813345  6.68641715  4.9195306  12.91421185 14.18413479]\n",
      " [ 4.12547462  4.05828945  7.63562266  5.56832807  8.35398301 10.18620962\n",
      "          nan  2.41788349  4.72788878  1.78859931  4.51992843  5.14843709\n",
      "   5.73920755 12.45782271  8.670988   13.29435626 13.38260556]\n",
      " [ 5.85388266  3.78274165  5.05386305  3.97300912 12.76648844 10.55981269\n",
      "   2.41788349         nan  1.9666769   1.22746061  5.48065109  4.53739313\n",
      "   5.11258065 11.78098498 10.94291686 12.13524928 11.72279677]\n",
      " [ 6.66925945  4.35312356  7.91640431  5.75000786 15.80225501 14.17414845\n",
      "   4.72788878  1.9666769          nan  5.38520077 12.31453227 11.61649301\n",
      "  12.71063373 21.26431879 17.93497364 17.77597534 17.04890728]\n",
      " [ 5.10053909  3.95560204  4.56496106  3.38703217  8.58056833  7.0102302\n",
      "   1.78859931  1.22746061  5.38520077         nan  1.89983582  1.40733186\n",
      "   1.90558915  6.66883526  5.35505375  9.37322615  9.58889185]\n",
      " [10.41192267  8.68966264  6.40708607  5.35686338 10.05302145  5.32766361\n",
      "   4.51992843  5.48065109 12.31453227  1.89983582         nan  0.43031063\n",
      "   0.74456873  2.10173895  2.20400462  6.82932734  7.41810157]\n",
      " [10.14269143  8.41752073  6.1975152   5.54157533 10.78993841  6.51479894\n",
      "   5.14843709  4.53739313 11.61649301  1.40733186  0.43031063         nan\n",
      "   0.1833422   2.18885282  2.97906964  7.21518205  7.66948618]\n",
      " [11.38004013  9.59916114  8.25796457  7.30715894 11.42371622  8.52813345\n",
      "   5.73920755  5.11258065 12.71063373  1.90558915  0.74456873  0.1833422\n",
      "          nan  2.40235831  3.28719008  6.39806383  6.70619938]\n",
      " [20.13060821 17.29957995 10.08345683 10.02550481 17.68171637  6.68641715\n",
      "  12.45782271 11.78098498 21.26431879  6.66883526  2.10173895  2.18885282\n",
      "   2.40235831         nan  3.32793639  8.14734095  8.73722868]\n",
      " [12.23400644 11.2842097  11.18767605  8.18718365  6.87306601  4.9195306\n",
      "   8.670988   10.94291686 17.93497364  5.35505375  2.20400462  2.97906964\n",
      "   3.28719008  3.32793639         nan  5.94851394  7.17858771]\n",
      " [19.58465814 14.50371161 17.41218045 12.44042238 13.80750718 12.91421185\n",
      "  13.29435626 12.13524928 17.77597534  9.37322615  6.82932734  7.21518205\n",
      "   6.39806383  8.14734095  5.94851394         nan  0.23225675]\n",
      " [20.9758087  15.68454149 18.33490882 13.414042   16.02018465 14.18413479\n",
      "  13.38260556 11.72279677 17.04890728  9.58889185  7.41810157  7.66948618\n",
      "   6.70619938  8.73722868  7.17858771  0.23225675         nan]]\n",
      "r: [[       nan 1.6669101  0.21967116 0.31672655 0.73247552 0.16676244\n",
      "  0.48407301 0.36190176 0.32361762 0.42056098 0.19435571 0.20092276\n",
      "  0.18276233 0.10399979 0.18059858 0.10467219 0.10546633]\n",
      " [1.6669101         nan 0.25163964 0.46577379 0.46801285 0.1599312\n",
      "  0.37792431 0.43757304 0.38937217 0.42516487 0.17955956 0.18706136\n",
      "  0.16840409 0.0942378  0.15474185 0.10939712 0.11150704]\n",
      " [0.21967116 0.25163964        nan 1.3453475  0.16086963 0.35741267\n",
      "  0.19082721 0.31235198 0.20442911 0.35162214 0.2315671  0.24170191\n",
      "  0.18647408 0.15407738 0.14922651 0.0867222  0.09120826]\n",
      " [0.31672655 0.46577379 1.3453475         nan 0.24222733 0.4873365\n",
      "  0.2577175  0.39178241 0.27761954 0.46740399 0.27285395 0.26633622\n",
      "  0.20772332 0.15277072 0.20122534 0.11960958 0.12302527]\n",
      " [0.73247552 0.46801285 0.16086963 0.24222733        nan 0.21788611\n",
      "  0.30527859 0.20928164 0.17159267 0.31447205 0.2563287  0.24014584\n",
      "  0.23049451 0.14969362 0.40196113 0.18853731 0.17262614]\n",
      " [0.16676244 0.1599312  0.35741267 0.4873365  0.21788611        nan\n",
      "  0.15854319 0.16443982 0.12531357 0.25149117 0.30811629 0.254163\n",
      "  0.19907826 0.25596702 0.37145158 0.12915169 0.129029  ]\n",
      " [0.48407301 0.37792431 0.19082721 0.2577175  0.30527859 0.15854319\n",
      "         nan 0.67614853 0.35419779 0.92888642 0.34069888 0.30188103\n",
      "  0.27811516 0.12922779 0.19902715 0.11781593 0.12916464]\n",
      " [0.36190176 0.43757304 0.31235198 0.39178241 0.20928164 0.16443982\n",
      "  0.67614853        nan 0.91327015 1.45251516 0.30314539 0.36931228\n",
      "  0.33596734 0.14696534 0.16880889 0.13908132 0.1578172 ]\n",
      " [0.32361762 0.38937217 0.20442911 0.27761954 0.17159267 0.12531357\n",
      "  0.35419779 0.91327015        nan 0.33845805 0.1381452  0.14767576\n",
      "  0.13826372 0.08329238 0.1052146  0.09718431 0.11084689]\n",
      " [0.42056098 0.42516487 0.35162214 0.46740399 0.31447205 0.25149117\n",
      "  0.92888642 1.45251516 0.33845805        nan 0.88849327 1.20957416\n",
      "  0.9153166  0.2636073  0.34991597 0.18289793 0.1957073 ]\n",
      " [0.19435571 0.17955956 0.2315671  0.27285395 0.2563287  0.30811629\n",
      "  0.34069888 0.30314539 0.1381452  0.88849327        nan 3.67362805\n",
      "  2.17944465 0.77863267 0.79507447 0.23324005 0.236603  ]\n",
      " [0.20092276 0.18706136 0.24170191 0.26633622 0.24014584 0.254163\n",
      "  0.30188103 0.36931228 0.14767576 1.20957416 3.67362805        nan\n",
      "  8.92880786 0.75416835 0.59301357 0.22274608 0.23070985]\n",
      " [0.18276233 0.16840409 0.18647408 0.20772332 0.23049451 0.19907826\n",
      "  0.27811516 0.33596734 0.13826372 0.9153166  2.17944465 8.92880786\n",
      "         nan 0.7046029  0.55018839 0.25774964 0.27010398]\n",
      " [0.10399979 0.0942378  0.15407738 0.15277072 0.14969362 0.25596702\n",
      "  0.12922779 0.14696534 0.08329238 0.2636073  0.77863267 0.75416835\n",
      "  0.7046029         nan 0.54757961 0.20409542 0.20888859]\n",
      " [0.18059858 0.15474185 0.14922651 0.20122534 0.40196113 0.37145158\n",
      "  0.19902715 0.16880889 0.1052146  0.34991597 0.79507447 0.59301357\n",
      "  0.55018839 0.54757961        nan 0.29901588 0.27038363]\n",
      " [0.10467219 0.10939712 0.0867222  0.11960958 0.18853731 0.12915169\n",
      "  0.11781593 0.13908132 0.09718431 0.18289793 0.23324005 0.22274608\n",
      "  0.25774964 0.20409542 0.29901588        nan 7.67037981]\n",
      " [0.10546633 0.11150704 0.09120826 0.12302527 0.17262614 0.129029\n",
      "  0.12916464 0.1578172  0.11084689 0.1957073  0.236603   0.23070985\n",
      "  0.27010398 0.20888859 0.27038363 7.67037981        nan]]\n",
      "Davies-Bouldin index per cluster: [1.6669101  1.6669101  1.3453475  1.3453475  0.73247552 0.4873365\n",
      " 0.92888642 1.45251516 0.91327015 1.45251516 3.67362805 8.92880786\n",
      " 8.92880786 0.77863267 0.79507447 7.67037981 7.67037981]\n",
      "Davies-Bouldin index total: 2.9668955668665244\n"
     ]
    }
   ],
   "source": [
    "cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(a, b):\n",
    "    if a < b:\n",
    "        return 1 - a/b\n",
    "    elif a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return b/a - 1\n",
    "    \n",
    "def cluster_performance_silhouette(df_total, values_scaled, clusters, source_stability, voltage_breakdown_selection, num_clusters):\n",
    "    mean_distances = np.array([np.array([np.sum(np.linalg.norm(values_scaled[cluster]-x, axis=1)) / len(cluster) for cluster in clusters]) for x in values_scaled])\n",
    "    optigrid_cluster = df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster']\n",
    "    selector = np.ones((len(values_scaled), num_clusters), dtype=bool)\n",
    "    selector[range(len(values)), optigrid_cluster] = False\n",
    "    print(mean_distances)\n",
    "    print(optigrid_cluster)\n",
    "    print(selector)\n",
    "    print(np.ma.masked_array(mean_distances, ~selector))\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'] = np.amin(np.ma.masked_array(mean_distances, selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'] = np.amin(np.ma.masked_array(mean_distances, ~selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'silhouette'] = np.vectorize(silhouette_coefficient)(df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'], df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs_euclid_squared_numpy(A, B):\n",
    "    sqrA = np.broadcast_to(np.sum(np.power(A, 2), 1).reshape(A.shape[0], 1), (A.shape[0], B.shape[0]))\n",
    "    sqrB = np.broadcast_to(np.sum(np.power(B, 2), 1).reshape(B.shape[0], 1), (B.shape[0], A.shape[0])).transpose()\n",
    "\n",
    "    return sqrA - 2*np.matmul(A, B.transpose()) + sqrB\n",
    "\n",
    "def cluster_performance_dbi(values_scaled, clusters, num_clusters):\n",
    "    print(\"values_scaled: {}\".format(values_scaled))\n",
    "    values_per_cluster = [np.take(values_scaled, c, axis=0) for c in clusters]\n",
    "    means = np.array([np.mean(c, axis=0) for c in values_per_cluster])\n",
    "    print(\"values_per_cluster: {}\".format(values_per_cluster[0][:10]))\n",
    "    print(\"means: {}\".format(means))\n",
    "    assigned_cluster_mean = np.zeros((len(values_scaled), len(values_scaled[0])))\n",
    "    for i, c in enumerate(clusters):\n",
    "        assigned_cluster_mean[c] = means[i]\n",
    "    print(\"assigned_cluster_mean: {}\".format(assigned_cluster_mean))\n",
    "        \n",
    "    dists_from_means = np.linalg.norm(values_scaled-assigned_cluster_mean, axis=1)\n",
    "    print(\"dists_from_means: {}\".format([dists_from_means[c] for c in clusters]))\n",
    "    s = np.array([np.sqrt(1./len(c) * np.sum(dists_from_means[c])) for c in clusters])\n",
    "    print(\"s: {}\".format(s))\n",
    "    \n",
    "    dists_between_clusters = all_pairs_euclid_squared_numpy(means, means)\n",
    "    np.fill_diagonal(dists_between_clusters, np.nan)\n",
    "    print(\"dists_between_clusters: {}\".format(dists_between_clusters))\n",
    "    \n",
    "    r = np.tile(s, (num_clusters, 1))\n",
    "    r = (r + r.T) / dists_between_clusters\n",
    "    print(\"r: {}\".format(r))\n",
    "    d = np.nanmax(r, axis=1)\n",
    "    dbi = np.mean(d)\n",
    "    print(\"Davies-Bouldin index per cluster: {}\".format(d))\n",
    "    print(\"Davies-Bouldin index total: {}\".format(dbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_clusters(optigrid, data, parameters):\n",
    "    values = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    result = pd.DataFrame(columns = pd.MultiIndex.from_tuples([(p, v) for p in parameters for v in values] + [('DENSITY', 'count'), ('DENSITY', 'percentage')]))\n",
    "    result.index.name = 'OPTIGRID_CLUSTER'\n",
    "    \n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        cluster_data = np.take(data, cluster, axis=0)\n",
    "        mean = np.mean(cluster_data, axis=0)\n",
    "        std = np.std(cluster_data, axis=0)\n",
    "        quantiles = np.quantile(cluster_data, [0, 0.25, 0.5, 0.75, 1], axis=0)\n",
    "        cluster_description = [[mean[i], std[i], quantiles[0][i], quantiles[1][i], quantiles[2][i], quantiles[3][i], quantiles[4][i]] for i in range(len(parameters))]\n",
    "        cluster_description = [item for sublist in cluster_description for item in sublist]\n",
    "        cluster_description.append(len(cluster))\n",
    "        cluster_description.append(len(cluster)/len(data)*100)\n",
    "        result.loc[i] = cluster_description\n",
    "    return result\n",
    "\n",
    "described = describe_clusters(optigrid, data, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('DENSITY', 'percentage')]\n",
    "\n",
    "num_of_clusters_to_print = 10\n",
    "described.sort_values(by=[('DENSITY', 'percentage')], ascending=False, inplace = True)\n",
    "print(\"Sum of densities of printed clusters: {:.1f}%\".format(described.head(n=num_of_clusters_to_print)[('DENSITY', 'percentage')].sum()))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the clusters we will plot the densities of the parameters. For comparability we will use explicit ranges for the x-axis per parameter. Those ranges should be chosen beforehand by an expert to validate or falsify his intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6 # number of clusters to visualize\n",
    "data = df[parameters].values # We select the unscaled data again, because by clustering we did not change any ordering and this data corresponds to the real world\n",
    "num_datapoints = len(data)\n",
    "\n",
    "resolution = 200\n",
    "bandwidth = [1, 0.01, 1, 10, 0.1, 0.001]\n",
    "num_kde_samples = 40000\n",
    "\n",
    "parameter_ranges = [[0,0] for i in range(len(parameters))]\n",
    "parameter_ranges[0] = [-300, -200] # Biasdisc x-axis\n",
    "\n",
    "parameter_ranges[1] = [5.1, 5.3] # Gas x-axis\n",
    "#parameter_ranges[2] = [0, 3] # High voltage current x-axis\n",
    "parameter_ranges[2] = [200, 300] # SolCen current x-axis\n",
    "#parameter_ranges[3] = [900, 2100] # Forwardpower x-axis\n",
    "parameter_ranges[3] = [1200, 1300] # SolExt current x-axis\n",
    "parameter_ranges[4] = [5, 20] # Oven1 power x-axis\n",
    "parameter_ranges[5] = [0, 0.05] # BCT25 current x-axis\n",
    "\n",
    "best_clusters = sorted(optigrid.clusters, key=lambda x: len(x), reverse=True)\n",
    "for i, cluster in enumerate(best_clusters[:num_clusters]):\n",
    "    median = [described.iloc[i,described.columns.get_loc((param, '50%'))] for param in parameters]\n",
    "    plot_cluster(data, cluster, parameters, parameter_ranges, resolution=resolution, median=median, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find all high voltage breakdowns that correspond to the currently considered source stability, and find out to which cluster each datapoint belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics.append(('num_of_breakdowns', ''))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics = [[(param, 'mean')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('num_of_breakdowns', '')]\n",
    "corr_described = described[wanted_statistics].corr()\n",
    "corr_described.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std'),  (param, 'min'),  (param, 'max')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist]\n",
    "df_breakdowns.groupby('is_breakdown').describe()[wanted_statistics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "size = 10\n",
    "data = np.random.uniform(0, 1, (size, 1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(np.linalg.norm(data-x, axis=1)) for x in data]) / (size - 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [3, 3],\n",
       "        [3, 3],\n",
       "        [1, 1]]), [[0, 6], [1, 2, 3], [4, 5]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "values = [0, 2, 2, 2, 3, 3, 1]\n",
    "values = np.array([[x, x] for x in values])\n",
    "clusters = [[0, 6], [1, 2, 3], [4, 5]]\n",
    "\n",
    "values, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values_scaled: [[0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [1 1]]\n",
      "values_per_cluster: [[0 0]\n",
      " [1 1]]\n",
      "means: [[0.5 0.5]\n",
      " [2.  2. ]\n",
      " [3.  3. ]]\n",
      "assigned_cluster_mean: [[0.5 0.5]\n",
      " [2.  2. ]\n",
      " [2.  2. ]\n",
      " [2.  2. ]\n",
      " [3.  3. ]\n",
      " [3.  3. ]\n",
      " [0.5 0.5]]\n",
      "dists_from_means: [array([0.70710678, 0.70710678]), array([0., 0., 0.]), array([0., 0.])]\n",
      "s: [0.84089642 0.         0.        ]\n",
      "dists_between_clusters: [[ nan  4.5 12.5]\n",
      " [ 4.5  nan  2. ]\n",
      " [12.5  2.   nan]]\n",
      "r: [[       nan 0.18686587 0.06727171]\n",
      " [0.18686587        nan 0.        ]\n",
      " [0.06727171 0.                nan]]\n",
      "Davies-Bouldin index per cluster: [0.18686587 0.18686587 0.06727171]\n",
      "Davies-Bouldin index total: 0.14700115111101975\n"
     ]
    }
   ],
   "source": [
    "cluster_performance_dbi(values, clusters, len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2888888888888889"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(values, [0, 1, 1, 1, 2, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering for source stability 1\n",
      "[[ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  1.00161365]\n",
      " ...\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -0.74528563]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]]\n",
      "[[ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  0.60128267]\n",
      " [ 1.68398476  0.90229536 -0.14205675  0.03117437 -0.10200334  1.00161365]\n",
      " ...\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -0.74528563]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]\n",
      " [-0.33085806 -1.93607809 -0.87421725  0.02690068 -1.37386619 -1.58234126]]\n",
      "Calculating cluster performance cluster performance\n"
     ]
    }
   ],
   "source": [
    "source_stable = 1\n",
    "print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "source_stability = df_total['source_stable'] == source_stable\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "print(values_scaled)\n",
    "optigrid = run_optigrid(values_scaled, optigrid_params) # and compute the clusters.\n",
    "print(values_scaled)\n",
    "#assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "print(\"Calculating cluster performance cluster performance\")\n",
    "#cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
