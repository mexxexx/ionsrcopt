{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preliminary tests showed, that Optigrid performed well on Data of November 2018. In this notebook we will explore a few other months and look for similarities and descrepancies in the results. We will use preprocessed data, where things like source stability and voltage breakdowns are indicated. Moreover, for now we will limit ourselfs to stable running sources, i.e. time periods with a low variance and a high current in the BCT25. We use the already preprocessed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module loading\n",
    "We use the Python modules from the ionsrcopt package that will be loaded in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data_from_csv(filenames, cols_to_read, rows_to_read):\n",
    "    \"\"\" Read a csv file into a DataFrame\n",
    "\n",
    "    Parameters:\n",
    "        filenames (list string): Filenames. Concatenates all into one data frame\n",
    "        cols_to_read (list of string): The column names to read, None if everything should be read\n",
    "        rows_to_read (list of int): The rown numbers to read, None if everything should be read\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(\"Loading data from csv file \\'{}\\'\".format(filename))\n",
    "\n",
    "        try:\n",
    "            if cols_to_read is None:\n",
    "                df = pd.read_csv(filename).fillna(method='ffill')\n",
    "            else:\n",
    "                df = pd.read_csv(filename, usecols=cols_to_read).fillna(method='ffill')\n",
    "        except:\n",
    "            print(\"File {} does not exist or is not a csv file\". format(filename))\n",
    "            exit()\n",
    "\n",
    "        if not ('Timestamp' in df.columns or 'Timestamp (UTC_TIME)' in df.columns):\n",
    "            print(\"No timestamp column was found. It must be named either \\'Timestamp\\' or \\'Timestamp (UTC_TIME)\\'.\")\n",
    "            exit()\n",
    "\n",
    "        df = df.rename(columns={'Timestamp (UTC_TIME)' : 'Timestamp'})\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp']) \n",
    "        df = df.set_index('Timestamp')\n",
    "        \n",
    "        if not rows_to_read is None:\n",
    "            df = df.iloc[rows_to_read].copy()\n",
    "\n",
    "        dfs.append(df)        \n",
    "\n",
    "    result = pd.concat(dfs, axis=0, sort=False)\n",
    "    return result.sort_index() \n",
    "\n",
    "def convert_column(df, column, type):\n",
    "    \"\"\" Converts the dtype of a column\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the column\n",
    "        column (string): The column name\n",
    "        type (string): dtype the column should be converted to\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The altered DataFrame or the old one, if it did not contain the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    if column in df.columns:\n",
    "        print(\"Converting column \\'{}\\' to \\'{}\\'\".format(column, type))\n",
    "        return df.astype({column:type})\n",
    "    else:\n",
    "        #print(\"Column \\'{}\\' does not exist\".format(column))\n",
    "        return df\n",
    "\n",
    "def convert_column_types(df):\n",
    "    \"\"\" Convert all columns of a Dataframe of measurements to single precision values.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame to be altered\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Started type conversion of columns...\")\n",
    "    df = convert_column(df, 'IP.NSRCGEN:BIASDISCAQNV', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:GASSASAQN', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:SOURCEHTAQNI', 'float32')\n",
    "    df = convert_column(df, 'IP.SAIREM2:FORWARDPOWER', 'float32')\n",
    "    df = convert_column(df, 'IP.NSRCGEN:OVEN1AQNP', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLCEN.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLEXT.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'IP.SOLINJ.ACQUISITION:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT15:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITF.BCT25:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITH.BCT41:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'ITL.BCT05:CURRENT', 'float32')\n",
    "    df = convert_column(df, 'source_stable', 'int32')\n",
    "    df = convert_column(df, 'is_breakdown', 'int32')\n",
    "    df = convert_column(df, 'duration_seconds', 'float32')\n",
    "    df = convert_column(df, 'optigrid_cluster', 'int32')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\" Clean the data of measurements, that are outliers, e.g. spikes in the extraction current.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the measurements.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Filtering data...\")\n",
    "    if 'ITF.BCT15:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT15:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITF.BCT25:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT25:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITH.BCT41:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT41:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'ITL.BCT05:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['ITF.BCT05:CURRENT'].apply(lambda x: np.nan if x < 0 else x)\n",
    "    if 'IP.NSRCGEN:OVEN1AQNP' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:OVEN1AQNP'].apply(lambda x: np.nan if x < 4.5 else x)\n",
    "    if 'IP.SOLEXT.ACQUISITION:CURRENT' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SOLEXT.ACQUISITION:CURRENT'].apply(lambda x: np.nan if x < 1200 else x)\n",
    "    if 'IP.NSRCGEN:BIASDISCAQNV' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:BIASDISCAQNV'].apply(lambda x: np.nan if x == 0 else x)\n",
    "    if 'IP.SAIREM2:FORWARDPOWER' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.SAIREM2:FORWARDPOWER'].apply(lambda x: np.nan if x < 500 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x > 2.5 else x)\n",
    "    if 'IP.NSRCGEN:SOURCEHTAQNI' in df.columns:\n",
    "        df['ITF.BCT25:CURRENT'] = df['IP.NSRCGEN:SOURCEHTAQNI'].apply(lambda x: np.nan if x < 0.5 else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../optigrid/grid_level.py\n",
    "class GridLevel:\n",
    "    \"\"\" Optigrid creates a nested partition of the input space. This data structure is used to represent a single level of the grid. Either it represents a cluster or it is devided further into subgrids \"\"\"\n",
    "\n",
    "    def __init__(self, cutting_planes, cluster_index):\n",
    "        \"\"\" Creates a grid level.\n",
    "\n",
    "        Parameters:\n",
    "            cutting_planes (list): The planes that are used to subdivide this grid level or None if it represents a cluster\n",
    "            cluster_index (int): The index of the represented cluster or None if it can be subdivided further\n",
    "        \"\"\"\n",
    "\n",
    "        self.cutting_planes = cutting_planes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.subgrids = []\n",
    "        self.subgrid_indices = []\n",
    "\n",
    "    def add_subgrid(self, subgrid_index, subgrid):\n",
    "        \"\"\" Add a deeper level to the grid\n",
    "\n",
    "        Parameters:\n",
    "            subgrid_index (int): For every cutting plane, the subgrid can lay either right or left. This information can be used to binary encode it all at once. This is the subgrid index\n",
    "            subgrid (GridLevel): The subgrid to add\n",
    "        \"\"\"\n",
    "\n",
    "        self.subgrid_indices.append(subgrid_index)\n",
    "        self.subgrids.append(subgrid)\n",
    "\n",
    "    def get_sublevel(self, datapoint):\n",
    "        \"\"\" For a given datapoint returns the subgrid it lies in\n",
    "        \n",
    "        Parameters: \n",
    "            datapoint (ndarray): The datapoint\n",
    "        \n",
    "        Returns:\n",
    "            GridLevel: The subgrid or -1 if it belongs to no subgrid, meaning the point is an outlier.\n",
    "        \"\"\"\n",
    "\n",
    "        if datapoint is None:\n",
    "            raise ValueError(\"Datapoint must not be None.\")\n",
    "\n",
    "        grid_index = 0\n",
    "        for i, cut in enumerate(self.cutting_planes):\n",
    "            if datapoint[cut[1]] > cut[0]:\n",
    "                grid_index += 2 ** i\n",
    "\n",
    "        if not grid_index in self.subgrid_indices:\n",
    "            return -1\n",
    "\n",
    "        return self.subgrids[self.subgrid_indices.index(grid_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../optigrid/optigrid.py\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "#from grid_level import GridLevel\n",
    "\n",
    "class Optigrid:\n",
    "    \"\"\" Implementation of the Optigrid Algorithm described in \"Optimal Grid-Clustering: Towards Breaking the Curse of Dimensionality in High-Dimensional Clustering\" by Hinneburg and Keim \"\"\"\n",
    "\n",
    "    def __init__(self, d, q, max_cut_score, noise_level, kde_bandwidth = None, kde_grid_ticks=100, kde_num_samples=15000, kde_atol=1E-6, kde_rtol=1E-4, verbose=False):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "            d (int): Dimension of the data\n",
    "            q (int): Number of cutting planes per iteration\n",
    "            max_cut_score (double): Maximum density of a cutting plane\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = d\n",
    "        self.q = q\n",
    "        self.max_cut_score = max_cut_score\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "        self.root = None\n",
    "        self.clusters = None\n",
    "        self.num_clusters = -1\n",
    "\n",
    "        self.kde_bandwidth = kde_bandwidth\n",
    "        self.kde_grid_ticks = kde_grid_ticks\n",
    "        self.kde_num_samples = kde_num_samples\n",
    "        self.kde_atol = kde_atol\n",
    "        self.kde_rtol = kde_rtol\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, data, weights=None):\n",
    "        \"\"\" Find all clusters in the data. Clusters are stored as indices pointing to the passed data, i.e. if '10' is in cluster '0' means, that data[10] is in cluster 0.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "        \"\"\"\n",
    "\n",
    "        data_count = len(data)\n",
    "        cluster_indices = np.array(range(data_count))\n",
    "\n",
    "        grid, clusters = self._iteration(data=data, weights=weights, cluster_indices=cluster_indices, percentage_of_values=1, last_cluster_name = [-1])\n",
    "        self.root = grid\n",
    "        self.clusters = clusters\n",
    "        self.num_clusters = len(clusters)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Optigrid found {} clusters.\".format(self.num_clusters))\n",
    "\n",
    "    def _iteration(self, data, weights, cluster_indices, percentage_of_values, last_cluster_name):\n",
    "        \"\"\" Do one recursive step of the optigrid algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "            current_cluster (int): (passed as list to be mutable) The last cluster name that was found, -1 if none\n",
    "\n",
    "        Returns:\n",
    "            GridLevel: The gridlevel at the current step with all its depth\n",
    "            list of list of int: All clusters in the current data chunk\n",
    "        \"\"\"\n",
    "\n",
    "        cuts_iteration = []\n",
    "        for i in range(self.d): # First create all best cuts\n",
    "            cuts_iteration += self._create_cuts_kde(data, cluster_indices, current_dimension=i, percentage_of_values=percentage_of_values, weights=weights)\n",
    "        \n",
    "        if not cuts_iteration:\n",
    "            last_cluster_name[0] += 1\n",
    "            if self.verbose:\n",
    "                print(\"Found cluster {}\".format(last_cluster_name[0]))\n",
    "\n",
    "            return GridLevel(cutting_planes=None, cluster_index=last_cluster_name[0]), [cluster_indices]\n",
    "    \n",
    "        cuts_iteration = sorted(cuts_iteration, key=lambda x: x[2])[:self.q] # Sort the cuts based on the density at the minima and select the q best ones\n",
    "        grid = GridLevel(cutting_planes=cuts_iteration, cluster_index=None)\n",
    "        \n",
    "        grid_data = self._fill_grid(data, cluster_indices, cuts_iteration) # Fill the subgrid based on the cuts\n",
    "    \n",
    "        result = []\n",
    "        for i, cluster in enumerate(grid_data):\n",
    "            if cluster.size==0:\n",
    "                continue\n",
    "            if self.verbose:\n",
    "                print(\"In current cluster: {:.2f}% of datapoints\".format(percentage_of_values*len(cluster)/len(cluster_indices)*100))\n",
    "            subgrid, subresult = self._iteration(data=data, weights=weights, cluster_indices=cluster, percentage_of_values=percentage_of_values*len(cluster)/len(cluster_indices), last_cluster_name=last_cluster_name) # Run Optigrid on every subgrid\n",
    "            grid.add_subgrid(i, subgrid)\n",
    "            result += subresult\n",
    "\n",
    "        return grid, result\n",
    "\n",
    "    def _fill_grid(self, data, cluster_indices, cuts):\n",
    "        \"\"\" Partitions the grid based on the selected cuts and assignes each cell the corresponding data points (as indices).\n",
    "        \n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            cuts (list): Cutting planes in the format (position, dimension, cutting_score)\n",
    "\n",
    "        Returns:\n",
    "            list of list of int: 2**num_cuts lists of indices representing the clusters in this level\n",
    "        \"\"\"\n",
    "        \n",
    "        num_cuts = len(cuts)\n",
    "        grid_index = np.zeros(len(cluster_indices))\n",
    "        for i, cut in enumerate(cuts):\n",
    "            cut_val = 2 ** i\n",
    "            grid_index[np.take(np.take(data, cut[1], axis=1), cluster_indices) > cut[0]] += cut_val\n",
    "\n",
    "        return [cluster_indices[grid_index==key] for key in range(2**num_cuts)]\n",
    "    \n",
    "    def _create_cuts_kde(self, data, cluster_indices, current_dimension, percentage_of_values, weights):\n",
    "        \"\"\" Find the best cuts in the specified dimension by estimating the data density using kde.\n",
    "\n",
    "        Parameters:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            current_dimension (int): Dimension on which to project\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "\n",
    "        Returns:\n",
    "            list: q best cuts in the format (position, dimension, cutting_score)\n",
    "        \"\"\"\n",
    "\n",
    "        grid, kde = self._estimate_distribution(data, cluster_indices, current_dimension, percentage_of_values=percentage_of_values, weights=weights) \n",
    "        kde = np.append(kde, 0)\n",
    "\n",
    "        peaks = self._find_peaks_distribution(kde)      \n",
    "        if not peaks:\n",
    "            return []\n",
    "\n",
    "        peaks = [peaks[0]] + sorted(sorted(peaks[1:-1], key=lambda x: kde[x], reverse=True)[:self.q - 1]) + [peaks[len(peaks) - 1]] # and get the q-1 most important peaks between the leftest and rightest one.\n",
    "        best_cuts = self._find_best_cuts(grid, kde, peaks, current_dimension)\n",
    "        return best_cuts\n",
    "\n",
    "    def _find_best_cuts(self, grid, kde, peaks, current_dimension):\n",
    "        \"\"\" Using a density estimate and its maxima, finds the best cutting planes\n",
    "        \n",
    "        Parameters:\n",
    "            grid (list of double): The grid on which the density estimate was evaluated\n",
    "            kde (list of double): For each point on the grid the corresponding density\n",
    "            peaks (list of double): The maxima of the density estimate on the grid\n",
    "            current_dimension (int): Dimension on which the data is projected\n",
    "\n",
    "        Returns:\n",
    "            list: Best cutting planes in this dimension in the format (position, dimension, cutting_score)\n",
    "        \"\"\"\n",
    "        best_cuts = [] \n",
    "        for i in range(len(peaks)-1): # between these peaks search for the optimal cutting plane\n",
    "            current_min = 1\n",
    "            current_min_index = -1\n",
    "            for j in range(peaks[i]+1, peaks[i+1]):\n",
    "                if kde[j] < current_min:\n",
    "                    current_min = kde[j]\n",
    "                    current_min_index = j\n",
    "            \n",
    "            if current_min_index >= 0 and current_min < self.max_cut_score:\n",
    "                best_cuts.append((grid[current_min_index], current_dimension, current_min)) # cutting plane format: (cutting coordinate, dimension in which we cut, density at minimum)\n",
    "        return best_cuts\n",
    "\n",
    "    def _find_peaks_distribution(self, kde):\n",
    "        \"\"\" Given a density distribution, locates its peaks\n",
    "\n",
    "        Parameters:\n",
    "            kde (list of double): The density estimates on an arbitrary 1D grid\n",
    "\n",
    "        Returns:\n",
    "            list of int: The corresponding indices of the grid where the kde has its peaks.\n",
    "        \"\"\"\n",
    "\n",
    "        peaks=[]\n",
    "        prev = 0\n",
    "        current = kde[0]\n",
    "        for bin in range(1, len(kde)): # Find all peaks that are above the noise level\n",
    "            next = kde[bin] \n",
    "            if current > prev and current > next and current >= self.noise_level:\n",
    "                peaks.append(bin-1)\n",
    "            prev = current\n",
    "            current = next\n",
    "        return peaks\n",
    "\n",
    "    def _estimate_distribution(self, data, cluster_indices, current_dimension, percentage_of_values, weights):\n",
    "        \"\"\" Estimate the distribution using a sample of the data projected to a coordinate axis using scikits kde estimate method\n",
    "\n",
    "        Parametes:\n",
    "            data (ndarray): Each datapoint has to be an array of d dimensions\n",
    "            cluster_indices (list of int): All indices that belong to the current cluster\n",
    "            current_dimension (int): Dimension on which to project\n",
    "            percentage_of_values (double): Percentage of values that lay in the current cluster (0-1)\n",
    "\n",
    "        Returns:\n",
    "            list of double: A equally spaced grid\n",
    "            list of double: The density on the grid points\n",
    "        \"\"\"\n",
    "\n",
    "        sample_size = min(self.kde_num_samples, len(cluster_indices))\n",
    "        sample = np.random.choice(cluster_indices, size=sample_size)\n",
    "        datapoints = data[sample][:,current_dimension]\n",
    "        #datapoints = np.expand_dims(datapoints, -1)\n",
    "        weights_sample = None\n",
    "        if not weights is None:\n",
    "            weights_sample = weights[sample]\n",
    "        min_val = np.amin(datapoints)\n",
    "        max_val = np.amax(datapoints)\n",
    "\n",
    "        #kde = KernelDensity(kernel=self.kde_kernel, bandwidth=self.kde_bandwidth, atol=self.kde_atol, rtol=self.kde_rtol).fit(datapoints)\n",
    "        std = datapoints.std(ddof=1)\n",
    "        if np.isclose(std, 0):\n",
    "            return 0, np.infty\n",
    "\n",
    "        kde = gaussian_kde(dataset=datapoints, bw_method=self.kde_bandwidth / std, weights=weights_sample)\n",
    "\n",
    "        grid = np.linspace(min_val, max_val, self.kde_grid_ticks)\n",
    "        #log_dens = kde.score_samples(grid)\n",
    "        dens = kde.evaluate(grid)\n",
    "        #return grid, np.exp(log_dens) * percentage_of_values\n",
    "        return grid, dens * percentage_of_values\n",
    "\n",
    "    def score_samples(self, samples):\n",
    "        \"\"\" For every sample calculates the cluster it belongs to\n",
    "\n",
    "        Parameters:\n",
    "            samples (list of ndarray): The sample to score. They need to have the same dimensionality and scale as the data optigrid was fitted with\n",
    "        \n",
    "        Returns:\n",
    "            list of int: For every sample, the cluster it belongs to or None if it is in no cluster (only possible for q>1)\n",
    "        \"\"\"\n",
    "\n",
    "        return [self._score_sample(sample) for sample in samples]\n",
    "\n",
    "    def _score_sample(self, sample):\n",
    "        \"\"\" Score a single sample\n",
    "\n",
    "        Parameters:\n",
    "            sample (ndarray): Needs to have the same dimensionality and scale as the data optigrid was fitted with\n",
    "\n",
    "        Returns:\n",
    "            int: Cluster the sample belongs to ore None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Optigrid needs to be fitted to a dataset first.\")\n",
    "\n",
    "        current_grid_level = self.root\n",
    "        while current_grid_level.cluster_index is None:\n",
    "            sub_level = current_grid_level.get_sublevel(sample)\n",
    "            if sub_level is None:\n",
    "                return None\n",
    "            \n",
    "            current_grid_level = sub_level\n",
    "\n",
    "        return current_grid_level.cluster_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specifiy all the columns we are interested in. There are three types: Parameters, these are the ones that will be clustered later on, Measurments and columns from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['Timestamp']\n",
    "parameters = ['IP.NSRCGEN:BIASDISCAQNV', 'IP.NSRCGEN:GASSASAQN', 'IP.SOLCEN.ACQUISITION:CURRENT', 'IP.SOLEXT.ACQUISITION:CURRENT', 'IP.NSRCGEN:OVEN1AQNP', 'ITF.BCT25:CURRENT']\n",
    "measurments = ['ITF.BCT25:CURRENT']\n",
    "preprocessing = ['source_stable', 'is_breakdown', 'duration_seconds']\n",
    "columns_to_load = time + parameters + measurments + preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the important files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../Data_Preprocessed/'\n",
    "input_files = ['Sep2018.csv', 'Nov2018.csv']\n",
    "input_paths = [input_folder + f for f in input_files]\n",
    "output_folder = '../Data_Clustered/'\n",
    "output_file = 'SepNov2018.csv'\n",
    "output_path = output_folder + output_file\n",
    "\n",
    "cluster_logfile = output_folder + 'cluster_runs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv file '../Data_Preprocessed/Sep2018.csv'\n",
      "Loading data from csv file '../Data_Preprocessed/Nov2018.csv'\n",
      "Started type conversion of columns...\n",
      "Converting column 'IP.NSRCGEN:BIASDISCAQNV' to 'float32'\n",
      "Converting column 'IP.NSRCGEN:GASSASAQN' to 'float32'\n",
      "Converting column 'IP.NSRCGEN:OVEN1AQNP' to 'float32'\n",
      "Converting column 'IP.SOLCEN.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'IP.SOLEXT.ACQUISITION:CURRENT' to 'float32'\n",
      "Converting column 'ITF.BCT25:CURRENT' to 'float32'\n",
      "Converting column 'source_stable' to 'int32'\n",
      "Converting column 'is_breakdown' to 'int32'\n",
      "Converting column 'duration_seconds' to 'float32'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                            26225856\n",
       "IP.NSRCGEN:BIASDISCAQNV          13112928\n",
       "IP.NSRCGEN:GASSASAQN             13112928\n",
       "IP.SOLCEN.ACQUISITION:CURRENT    13112928\n",
       "IP.SOLEXT.ACQUISITION:CURRENT    13112928\n",
       "IP.NSRCGEN:OVEN1AQNP             13112928\n",
       "ITF.BCT25:CURRENT                13112928\n",
       "duration_seconds                 13112928\n",
       "source_stable                    13112928\n",
       "is_breakdown                     13112928\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = read_data_from_csv(input_paths, columns_to_load, None)\n",
    "df_total.dropna(inplace=True)\n",
    "df_total = convert_column_types(df_total)\n",
    "df_total.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3278232, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select what data we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_values(df_total, parameters, source_stability, voltage_breakdown):\n",
    "    data = df_total.loc[source_stability & voltage_breakdown, parameters].values\n",
    "    weights = df_total.loc[source_stability & voltage_breakdown, 'duration_seconds'].values\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready we can begin clustering. But first we standard scale it, so that all parameters have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def scale_values(values, scaler):\n",
    "    if not scaler:\n",
    "        scaler = preprocessing.StandardScaler().fit(values)\n",
    "    values_scaled = scaler.transform(values)\n",
    "    return scaler, values_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for optigrid can be chosen by visually examening the distribution of normalized data, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(parameters)\n",
    "q=1\n",
    "max_cut_score = 0.3\n",
    "noise_level = 0.1\n",
    "\n",
    "optigrid_params = {\n",
    "    'd' : d, \n",
    "    'q' : q, \n",
    "    'max_cut_score' : max_cut_score, \n",
    "    'noise_level' : noise_level,\n",
    "    'kde_bandwidth' : 0.1,\n",
    "    'verbose' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_optigrid(values_scaled, weights, optigrid_params):\n",
    "    optigrid = Optigrid(**optigrid_params)\n",
    "    optigrid.fit(values_scaled, weights)\n",
    "    return optigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the clusters are found, we set an according column in the original dataframe containing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_df_total(df_total, optigrid, num_values, source_stability, voltage_breakdown_selection):\n",
    "    clusters = np.zeros(num_values)\n",
    "\n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        clusters[cluster] = i\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we bundle all these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df_total, parameters, source_stable, optigrid_params):\n",
    "    print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "    source_stability = df_total['source_stable'] == source_stable\n",
    "    voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "    \n",
    "    values, weights = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "    scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "    optigrid = run_optigrid(values_scaled, weights, optigrid_params) # and compute the clusters.\n",
    "    assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "    #print(\"Calculating cluster performance cluster performance\")\n",
    "    #cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)\n",
    "    #cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)\n",
    "    \n",
    "    print(\"Scoring voltage breakdowns\")\n",
    "    values, weights = select_values(df_total, parameters, source_stability, voltage_breakdown_selection) # Now, get the datapoints when the voltage broke down\n",
    "    _, values_scaled = scale_values(values, scaler) # scale it to the same ranges\n",
    "    scored_samples = optigrid.score_samples(values_scaled) # and find the corresponding clusters.\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster'] = scored_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering for source stability 1\n",
      "In current cluster: 77.75% of datapoints\n",
      "In current cluster: 11.92% of datapoints\n",
      "In current cluster: 6.82% of datapoints\n",
      "Found cluster 0\n",
      "In current cluster: 5.10% of datapoints\n",
      "Found cluster 1\n",
      "In current cluster: 65.83% of datapoints\n",
      "In current cluster: 27.48% of datapoints\n",
      "In current cluster: 24.24% of datapoints\n",
      "In current cluster: 7.45% of datapoints\n",
      "In current cluster: 2.79% of datapoints\n",
      "Found cluster 2\n",
      "In current cluster: 4.67% of datapoints\n",
      "Found cluster 3\n",
      "In current cluster: 16.78% of datapoints\n",
      "In current cluster: 2.98% of datapoints\n",
      "Found cluster 4\n",
      "In current cluster: 13.80% of datapoints\n",
      "In current cluster: 10.92% of datapoints\n",
      "Found cluster 5\n",
      "In current cluster: 2.88% of datapoints\n",
      "Found cluster 6\n",
      "In current cluster: 3.25% of datapoints\n",
      "Found cluster 7\n",
      "In current cluster: 38.34% of datapoints\n",
      "In current cluster: 19.53% of datapoints\n",
      "In current cluster: 9.90% of datapoints\n",
      "In current cluster: 7.04% of datapoints\n",
      "Found cluster 8\n",
      "In current cluster: 2.86% of datapoints\n",
      "Found cluster 9\n",
      "In current cluster: 9.63% of datapoints\n",
      "Found cluster 10\n",
      "In current cluster: 18.82% of datapoints\n",
      "In current cluster: 13.15% of datapoints\n",
      "In current cluster: 8.89% of datapoints\n",
      "Found cluster 11\n",
      "In current cluster: 4.26% of datapoints\n",
      "Found cluster 12\n",
      "In current cluster: 5.67% of datapoints\n",
      "Found cluster 13\n",
      "In current cluster: 22.25% of datapoints\n",
      "In current cluster: 3.56% of datapoints\n",
      "Found cluster 14\n",
      "In current cluster: 18.69% of datapoints\n",
      "Found cluster 15\n",
      "Optigrid found 16 clusters.\n",
      "Scoring voltage breakdowns\n",
      "Starting clustering for source stability 0\n",
      "In current cluster: 92.75% of datapoints\n",
      "In current cluster: 74.52% of datapoints\n",
      "In current cluster: 47.49% of datapoints\n",
      "In current cluster: 30.72% of datapoints\n",
      "In current cluster: 11.14% of datapoints\n",
      "In current cluster: 8.32% of datapoints\n",
      "Found cluster 0\n",
      "In current cluster: 2.83% of datapoints\n",
      "Found cluster 1\n",
      "In current cluster: 19.58% of datapoints\n",
      "In current cluster: 12.73% of datapoints\n",
      "In current cluster: 3.23% of datapoints\n",
      "Found cluster 2\n",
      "In current cluster: 9.50% of datapoints\n",
      "Found cluster 3\n",
      "In current cluster: 6.85% of datapoints\n",
      "Found cluster 4\n",
      "In current cluster: 16.77% of datapoints\n",
      "In current cluster: 7.50% of datapoints\n",
      "Found cluster 5\n",
      "In current cluster: 9.27% of datapoints\n",
      "Found cluster 6\n",
      "In current cluster: 27.03% of datapoints\n",
      "In current cluster: 20.85% of datapoints\n",
      "In current cluster: 13.51% of datapoints\n",
      "Found cluster 7\n",
      "In current cluster: 7.34% of datapoints\n",
      "Found cluster 8\n",
      "In current cluster: 6.19% of datapoints\n",
      "Found cluster 9\n",
      "In current cluster: 18.23% of datapoints\n",
      "In current cluster: 6.53% of datapoints\n",
      "Found cluster 10\n",
      "In current cluster: 11.69% of datapoints\n",
      "Found cluster 11\n",
      "In current cluster: 7.25% of datapoints\n",
      "Found cluster 12\n",
      "Optigrid found 13 clusters.\n",
      "Scoring voltage breakdowns\n"
     ]
    }
   ],
   "source": [
    "df_total['optigrid_cluster'] = np.nan\n",
    "cluster(df_total, parameters, 1, optigrid_params)\n",
    "cluster(df_total, parameters, 0, optigrid_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long term storage\n",
    "We will save the clustered data to a file.\n",
    "\n",
    "First, create the logging string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "logstring = \"[{}] \\'{}\\' cluster results saved to \\'{}\\'. Columns used: {}. Parameters used: {}\\n\".format(dt_string, input_paths, output_path, parameters, optigrid_params)\n",
    "with open(cluster_logfile, \"a\") as myfile:\n",
    "    myfile.write(logstring)\n",
    "\n",
    "logstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.astype({'optigrid_cluster' : 'int64'})\n",
    "df_total[df_total.shift(1)==df_total] = np.nan\n",
    "df_total.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "The selection of the parameters for optigrid can be done by looking of the densities of the normalized parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_stability = df_total['source_stable'] == 1\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection)\n",
    "_, values_scaled = scale_values(values, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (50,10)\n",
    "\n",
    "def estimate_distribution(data, cluster_indices, current_dimension, num_steps, bandwidth = 0.1, percentage_of_values=1, num_kde_samples=15000):\n",
    "    sample_size = min(num_kde_samples, len(cluster_indices))\n",
    "    sample = np.random.choice(cluster_indices, size=sample_size)\n",
    "    datapoints = np.expand_dims(data[sample][:,current_dimension], -1)\n",
    "    min_val = np.amin(datapoints)\n",
    "    max_val = np.amax(datapoints)\n",
    "    grid = np.linspace([min_val], [max_val], num_steps)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth, atol=1E-6, rtol=1E-4).fit(datapoints)\n",
    "    log_dens = kde.score_samples(grid)\n",
    "    return grid, np.exp(log_dens) * percentage_of_values\n",
    "\n",
    "def plot_cluster(data, cluster_indices, parameters, parameter_ranges, median, resolution, bandwidth, percentage_of_values, num_kde_samples):\n",
    "    if isinstance(bandwidth, float):\n",
    "        bandwidth = [bandwidth for i in range(len(parameters))]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.title('Densities of specified parameters')\n",
    "    \n",
    "    for i, parameter in enumerate(parameters):\n",
    "        grid, kde = estimate_distribution(data, cluster_indices, i, resolution, bandwidth=bandwidth[i], percentage_of_values=percentage_of_values, num_kde_samples=num_kde_samples)\n",
    "        ax = plt.subplot('1{}{}'.format(len(parameters), i+1))\n",
    "        ax.set_title(\"{}\".format(parameter), fontsize=22)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "        if parameter_ranges:\n",
    "            ax.set_xlim(*parameter_ranges[i])\n",
    "            #ax.set_ylim(*parameter_ranges[i][1])\n",
    "            \n",
    "        if median is not None:\n",
    "            ax.axvline(x=median[i], color='red')\n",
    "        \n",
    "        ax.plot(grid, kde)\n",
    "    \n",
    "    plt.suptitle('Densities of specified parameters', fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_indices = np.array(range(0, len(values_scaled)))\n",
    "resolution = 200\n",
    "bandwidth = 0.2\n",
    "\n",
    "num_kde_samples = 40000\n",
    "\n",
    "plot_cluster(values_scaled, cluster_indices, parameters, parameter_ranges=None, median=None, resolution=resolution, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these graphs we chose the noise level to be 0.1 and the max_cut_score, i.e. the maximum density where we still do a cut between two peaks, as 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(a, b):\n",
    "    if a < b:\n",
    "        return 1 - a/b\n",
    "    elif a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return b/a - 1\n",
    "    \n",
    "def cluster_performance_silhouette(df_total, values_scaled, clusters, source_stability, voltage_breakdown_selection, num_clusters):\n",
    "    mean_distances = np.array([np.array([np.sum(np.linalg.norm(values_scaled[cluster]-x, axis=1)) / len(cluster) for cluster in clusters]) for x in values_scaled])\n",
    "    optigrid_cluster = df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster']\n",
    "    selector = np.ones((len(values_scaled), num_clusters), dtype=bool)\n",
    "    selector[range(len(values)), optigrid_cluster] = False\n",
    "    print(mean_distances)\n",
    "    print(optigrid_cluster)\n",
    "    print(selector)\n",
    "    print(np.ma.masked_array(mean_distances, ~selector))\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'] = np.amin(np.ma.masked_array(mean_distances, selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'] = np.amin(np.ma.masked_array(mean_distances, ~selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'silhouette'] = np.vectorize(silhouette_coefficient)(df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'], df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs_euclid_squared_numpy(A, B):\n",
    "    sqrA = np.broadcast_to(np.sum(np.power(A, 2), 1).reshape(A.shape[0], 1), (A.shape[0], B.shape[0]))\n",
    "    sqrB = np.broadcast_to(np.sum(np.power(B, 2), 1).reshape(B.shape[0], 1), (B.shape[0], A.shape[0])).transpose()\n",
    "\n",
    "    return sqrA - 2*np.matmul(A, B.transpose()) + sqrB\n",
    "\n",
    "def cluster_performance_dbi(values_scaled, clusters, num_clusters):\n",
    "    print(\"values_scaled: {}\".format(values_scaled))\n",
    "    values_per_cluster = [np.take(values_scaled, c, axis=0) for c in clusters]\n",
    "    means = np.array([np.mean(c, axis=0) for c in values_per_cluster])\n",
    "    print(\"values_per_cluster: {}\".format(values_per_cluster[0][:10]))\n",
    "    print(\"means: {}\".format(means))\n",
    "    assigned_cluster_mean = np.zeros((len(values_scaled), len(values_scaled[0])))\n",
    "    for i, c in enumerate(clusters):\n",
    "        assigned_cluster_mean[c] = means[i]\n",
    "    print(\"assigned_cluster_mean: {}\".format(assigned_cluster_mean))\n",
    "        \n",
    "    dists_from_means = np.linalg.norm(values_scaled-assigned_cluster_mean, axis=1)\n",
    "    print(\"dists_from_means: {}\".format([dists_from_means[c] for c in clusters]))\n",
    "    s = np.array([np.sqrt(1./len(c) * np.sum(dists_from_means[c])) for c in clusters])\n",
    "    print(\"s: {}\".format(s))\n",
    "    \n",
    "    dists_between_clusters = all_pairs_euclid_squared_numpy(means, means)\n",
    "    np.fill_diagonal(dists_between_clusters, np.nan)\n",
    "    print(\"dists_between_clusters: {}\".format(dists_between_clusters))\n",
    "    \n",
    "    r = np.tile(s, (num_clusters, 1))\n",
    "    r = (r + r.T) / dists_between_clusters\n",
    "    print(\"r: {}\".format(r))\n",
    "    d = np.nanmax(r, axis=1)\n",
    "    dbi = np.mean(d)\n",
    "    print(\"Davies-Bouldin index per cluster: {}\".format(d))\n",
    "    print(\"Davies-Bouldin index total: {}\".format(dbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_clusters(optigrid, data, parameters):\n",
    "    values = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    result = pd.DataFrame(columns = pd.MultiIndex.from_tuples([(p, v) for p in parameters for v in values] + [('DENSITY', 'count'), ('DENSITY', 'percentage')]))\n",
    "    result.index.name = 'OPTIGRID_CLUSTER'\n",
    "    \n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        cluster_data = np.take(data, cluster, axis=0)\n",
    "        mean = np.mean(cluster_data, axis=0)\n",
    "        std = np.std(cluster_data, axis=0)\n",
    "        quantiles = np.quantile(cluster_data, [0, 0.25, 0.5, 0.75, 1], axis=0)\n",
    "        cluster_description = [[mean[i], std[i], quantiles[0][i], quantiles[1][i], quantiles[2][i], quantiles[3][i], quantiles[4][i]] for i in range(len(parameters))]\n",
    "        cluster_description = [item for sublist in cluster_description for item in sublist]\n",
    "        cluster_description.append(len(cluster))\n",
    "        cluster_description.append(len(cluster)/len(data)*100)\n",
    "        result.loc[i] = cluster_description\n",
    "    return result\n",
    "\n",
    "described = describe_clusters(optigrid, data, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('DENSITY', 'percentage')]\n",
    "\n",
    "num_of_clusters_to_print = 10\n",
    "described.sort_values(by=[('DENSITY', 'percentage')], ascending=False, inplace = True)\n",
    "print(\"Sum of densities of printed clusters: {:.1f}%\".format(described.head(n=num_of_clusters_to_print)[('DENSITY', 'percentage')].sum()))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the clusters we will plot the densities of the parameters. For comparability we will use explicit ranges for the x-axis per parameter. Those ranges should be chosen beforehand by an expert to validate or falsify his intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6 # number of clusters to visualize\n",
    "data = df[parameters].values # We select the unscaled data again, because by clustering we did not change any ordering and this data corresponds to the real world\n",
    "num_datapoints = len(data)\n",
    "\n",
    "resolution = 200\n",
    "bandwidth = [1, 0.01, 1, 10, 0.1, 0.001]\n",
    "num_kde_samples = 40000\n",
    "\n",
    "parameter_ranges = [[0,0] for i in range(len(parameters))]\n",
    "parameter_ranges[0] = [-300, -200] # Biasdisc x-axis\n",
    "\n",
    "parameter_ranges[1] = [5.1, 5.3] # Gas x-axis\n",
    "#parameter_ranges[2] = [0, 3] # High voltage current x-axis\n",
    "parameter_ranges[2] = [200, 300] # SolCen current x-axis\n",
    "#parameter_ranges[3] = [900, 2100] # Forwardpower x-axis\n",
    "parameter_ranges[3] = [1200, 1300] # SolExt current x-axis\n",
    "parameter_ranges[4] = [5, 20] # Oven1 power x-axis\n",
    "parameter_ranges[5] = [0, 0.05] # BCT25 current x-axis\n",
    "\n",
    "best_clusters = sorted(optigrid.clusters, key=lambda x: len(x), reverse=True)\n",
    "for i, cluster in enumerate(best_clusters[:num_clusters]):\n",
    "    median = [described.iloc[i,described.columns.get_loc((param, '50%'))] for param in parameters]\n",
    "    plot_cluster(data, cluster, parameters, parameter_ranges, resolution=resolution, median=median, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find all high voltage breakdowns that correspond to the currently considered source stability, and find out to which cluster each datapoint belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics.append(('num_of_breakdowns', ''))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics = [[(param, 'mean')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('num_of_breakdowns', '')]\n",
    "corr_described = described[wanted_statistics].corr()\n",
    "corr_described.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std'),  (param, 'min'),  (param, 'max')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist]\n",
    "df_breakdowns.groupby('is_breakdown').describe()[wanted_statistics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "size = 10\n",
    "data = np.random.uniform(0, 1, (size, 1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(np.linalg.norm(data-x, axis=1)) for x in data]) / (size - 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "values = [0, 2, 2, 2, 3, 3, 1]\n",
    "values = np.array([[x, x] for x in values])\n",
    "clusters = [[0, 6], [1, 2, 3], [4, 5]]\n",
    "\n",
    "values, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values, clusters, len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(values, [0, 1, 1, 1, 2, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_stable = 1\n",
    "print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "source_stability = df_total['source_stable'] == source_stable\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "print(values_scaled)\n",
    "optigrid = run_optigrid(values_scaled, optigrid_params) # and compute the clusters.\n",
    "print(values_scaled)\n",
    "#assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "print(\"Calculating cluster performance cluster performance\")\n",
    "#cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
