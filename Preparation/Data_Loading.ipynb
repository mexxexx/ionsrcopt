{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytimber\n",
    "ldb = pytimber.LoggingDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the output file and the times you want to download. Timber and pyTimber conflict with regards to the the times, probably because of winter and summertime. If you want to have data stored in Timber from 00:00 to 01:00, you might to request either from 01:00 to 02:00 or even 02:00 to 03:00. We have to account for this shift later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/source_features.py\n",
    "class SourceFeatures(object):\n",
    "    TIMESTAMP = 'UTC_TIME'\n",
    "    BIASDISCAQNV = 'IP.NSRCGEN:BIASDISCAQNV'\n",
    "    GASAQN = 'IP.NSRCGEN:GASAQN'\n",
    "    GASSASAQN = 'IP.NSRCGEN:GASSASAQN'\n",
    "    SOLINJ_CURRENT = 'IP.SOLINJ.ACQUISITION:CURRENT'\n",
    "    SOLCEN_CURRENT = 'IP.SOLCEN.ACQUISITION:CURRENT'\n",
    "    SOLEXT_CURRENT = 'IP.SOLEXT.ACQUISITION:CURRENT'\n",
    "    OVEN1AQNP = 'IP.NSRCGEN:OVEN1AQNP'\n",
    "    OVEN2AQNP = 'IP.NSRCGEN:OVEN2AQNP'\n",
    "    SOURCEHTAQNI = 'IP.NSRCGEN:SOURCEHTAQNI'\n",
    "    SOURCEHTAQNV = 'IP.NSRCGEN:SOURCEHTAQNV'\n",
    "    SAIREM2_FORWARDPOWER = 'IP.SAIREM2:FORWARDPOWER'\n",
    "    THOMSON_FORWARDPOWER = 'IP.NSRCGEN:RFTHOMSONAQNFWD'\n",
    "    SPARK_COUNTER = 'IP.NSRCGEN:SPARKS'\n",
    "    BCT05_CURRENT = 'ITL.BCT05:CURRENT'\n",
    "    BCT25_CURRENT = 'ITF.BCT25:CURRENT'\n",
    "    BCT41_CURRENT = 'ITH.BCT41:CURRENT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select all parameters you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(parameters_raw, parameters_scaled, t1, t2):\n",
    "    print(\"Loading Data in interval {} to {}\".format(t1, t2))\n",
    "    result = {}\n",
    "\n",
    "    if parameters_raw:\n",
    "        result = ldb.get(parameters_raw, t1, t2, unixtime=True)\n",
    "\n",
    "    for k, v in parameters_scaled.items():\n",
    "        data = ldb.getScaled(k, t1, t2, scaleAlgorithm=v['scale'], scaleInterval=v['interval'], scaleSize=v['size'], unixtime=True)\n",
    "        result.update(data)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "def load_existing_data(filename, replace_column):\n",
    "    if not path.exists(filename):\n",
    "        print(\"The file {} does not yet exist, we will create a new one\".format(filename))\n",
    "        return pd.DataFrame(columns=[SourceFeatures.TIMESTAMP])\n",
    "    \n",
    "    print(\"Loading data from {}.\".format(filename))\n",
    "    if replace_column:\n",
    "        print(\"We will replace columns that already exist\")\n",
    "    else:\n",
    "        print(\"We will only append new columns\")\n",
    "        \n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def create_base_df(filename, replace_file, replace_column):\n",
    "    if replace_file:\n",
    "        df = pd.DataFrame(columns=[SourceFeatures.TIMESTAMP])\n",
    "    else:\n",
    "        df = load_existing_data(filename, replace_column)\n",
    "\n",
    "    df.set_index(SourceFeatures.TIMESTAMP, inplace = True)\n",
    "    df.index = pd.to_datetime(df.index).tz_localize('UTC')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_duplicate_times(time_series):\n",
    "    x = time_series.duplicated()\n",
    "    count = x[x].count()\n",
    "    if count > 0:\n",
    "        print(\"Time duplicates exist!\")\n",
    "\n",
    "def join_result(df, result, replace_column):\n",
    "    print(\"Joining together result\")\n",
    "    for parameter, values in result.items():\n",
    "        print(\"For column {} {} datapoints exist.\".format(parameter, len(values[1])))\n",
    "\n",
    "        if parameter in df.columns:\n",
    "            print(\"Parameter {} is already in the data frame. There it has {} values. In the newly retrieved dataset it has {} values.\".format(parameter, df[parameter].count(), len(values[1])))\n",
    "            if not replace_column:\n",
    "                print(\"Skipping.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Removing old column.\")\n",
    "                df = df.drop(parameter, axis=1)\n",
    "                df = df.dropna(axis=0, how='all')\n",
    "\n",
    "        df_column = pd.DataFrame(columns=[SourceFeatures.TIMESTAMP, parameter])\n",
    "        df_column[SourceFeatures.TIMESTAMP] = pd.Series([datetime.fromtimestamp(timestamp, tz=pytz.utc) for timestamp in values[0]])\n",
    "        check_duplicate_times(df_column[SourceFeatures.TIMESTAMP])\n",
    "        df_column[parameter] = values[1]\n",
    "\n",
    "        df_column.set_index(SourceFeatures.TIMESTAMP, inplace = True)\n",
    "        df_column.dropna(inplace=True)\n",
    "\n",
    "        df = df.join(df_column, how='outer')\n",
    "\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.index = df.index.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
    "    df.index.name = SourceFeatures.TIMESTAMP\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the timeindex is duplicated, we will only keep the first occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the output to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, filename):\n",
    "    print(\"Saving result to {}\".format(filename))\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, t1, t2, parameters_raw, parameters_scaled, replace_file, replace_column):\n",
    "    result = get_result(parameters_raw, parameters_scaled, t1, t2)\n",
    "    df = create_base_df(filename, replace_file, replace_column)\n",
    "    df = join_result(df, result, replace_column)\n",
    "    df = df[~df.index.duplicated(keep='first')].copy()\n",
    "    save_df(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_raw = [\n",
    "        #SourceFeatures.BIASDISCAQNV, \n",
    "        #SourceFeatures.GASAQN, \n",
    "        #SourceFeatures.OVEN1AQNP,\n",
    "        #SourceFeatures.OVEN2AQNP,\n",
    "        #SourceFeatures.SOLINJ_CURRENT,\n",
    "        #SourceFeatures.SOLCEN_CURRENT,\n",
    "        #SourceFeatures.SOLEXT_CURRENT,\n",
    "        #SourceFeatures.SOURCEHTAQNI,\n",
    "        #SourceFeatures.BCT25_CURRENT,\n",
    "        #SourceFeatures.BCT41_CURRENT,\n",
    "        #SourceFeatures.SOURCEHTAQNV,\n",
    "        #SourceFeatures.BCT05_CURRENT,\n",
    "        SourceFeatures.SPARK_COUNTER,\n",
    "]\n",
    "parameters_scaled = {\n",
    "        #SourceFeatures.THOMSON_FORWARDPOWER : {'scale' : 'AVG', 'interval' : 'SECOND', 'size' : '10'},\n",
    "        #SourceFeatures.BCT05_CURRENT : {'scale' : 'AVG', 'interval' : 'MINUTE', 'size' : '2'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def load_data(filename, year, month, replace_file, replace_column):\n",
    "    t1 = '{}-{:02d}-01 00:00:00.000'.format(year, month)\n",
    "    if month == 12:\n",
    "        month = 0\n",
    "        year += 1\n",
    "    \n",
    "    t2 = '{}-{:02d}-01 00:00:00.000'.format(year, month+1)\n",
    "\n",
    "    t1 = pytz.utc.localize(datetime.strptime(t1, '%Y-%m-%d %H:%M:%S.%f')).astimezone(tz=None)\n",
    "    t2 = pytz.utc.localize(datetime.strptime(t2, '%Y-%m-%d %H:%M:%S.%f')).astimezone(tz=None)\n",
    "    \n",
    "    get_data(filename, t1, t2, parameters_raw, parameters_scaled, replace_file, replace_column)\n",
    "    print(\"Finished download of data {}/{}\\n\".format(month, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data in interval 2018-01-01 01:00:00+01:00 to 2018-02-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Jan2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 742 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jan2018.csv\n",
      "Finished download of data 1/2018\n",
      "\n",
      "Loading Data in interval 2018-02-01 01:00:00+01:00 to 2018-03-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Feb2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 618 datapoints exist.\n",
      "Saving result to ../Data_Raw/Feb2018.csv\n",
      "Finished download of data 2/2018\n",
      "\n",
      "Loading Data in interval 2018-03-01 01:00:00+01:00 to 2018-04-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Mar2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1474 datapoints exist.\n",
      "Saving result to ../Data_Raw/Mar2018.csv\n",
      "Finished download of data 3/2018\n",
      "\n",
      "Loading Data in interval 2018-04-01 02:00:00+02:00 to 2018-05-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Apr2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1033 datapoints exist.\n",
      "Saving result to ../Data_Raw/Apr2018.csv\n",
      "Finished download of data 4/2018\n",
      "\n",
      "Loading Data in interval 2018-05-01 02:00:00+02:00 to 2018-06-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/May2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1299 datapoints exist.\n",
      "Saving result to ../Data_Raw/May2018.csv\n",
      "Finished download of data 5/2018\n",
      "\n",
      "Loading Data in interval 2018-06-01 02:00:00+02:00 to 2018-07-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Jun2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1307 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jun2018.csv\n",
      "Finished download of data 6/2018\n",
      "\n",
      "Loading Data in interval 2018-07-01 02:00:00+02:00 to 2018-08-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Jul2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1334 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jul2018.csv\n",
      "Finished download of data 7/2018\n",
      "\n",
      "Loading Data in interval 2018-08-01 02:00:00+02:00 to 2018-09-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Aug2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1123 datapoints exist.\n",
      "Saving result to ../Data_Raw/Aug2018.csv\n",
      "Finished download of data 8/2018\n",
      "\n",
      "Loading Data in interval 2018-09-01 02:00:00+02:00 to 2018-10-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Sep2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 946 datapoints exist.\n",
      "Saving result to ../Data_Raw/Sep2018.csv\n",
      "Finished download of data 9/2018\n",
      "\n",
      "Loading Data in interval 2018-10-01 02:00:00+02:00 to 2018-11-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Oct2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 912 datapoints exist.\n",
      "Saving result to ../Data_Raw/Oct2018.csv\n",
      "Finished download of data 10/2018\n",
      "\n",
      "Loading Data in interval 2018-11-01 01:00:00+01:00 to 2018-12-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Nov2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SPARKS 1030 datapoints exist.\n",
      "Saving result to ../Data_Raw/Nov2018.csv\n",
      "Finished download of data 11/2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_folder = '../Data_Raw/'\n",
    "\n",
    "year = 2018\n",
    "start_month = 'Jan'\n",
    "end_month = 'Nov'\n",
    "\n",
    "replace_file = False\n",
    "replace_column = True\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for m in months[months.index(start_month):months.index(end_month)+1]:\n",
    "    filename = output_folder + '{}{}.csv'.format(m, year)\n",
    "    load_data(filename, year, months.index(m)+1, replace_file, replace_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
