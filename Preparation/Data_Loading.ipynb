{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytimber\n",
    "ldb = pytimber.LoggingDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the output file and the times you want to download. Timber and pyTimber conflict with regards to the the times, probably because of winter and summertime. If you want to have data stored in Timber from 00:00 to 01:00, you might to request either from 01:00 to 02:00 or even 02:00 to 03:00. We have to account for this shift later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../ionsrcopt/source_features.py\n",
    "class SourceFeatures(object):\n",
    "    TIMESTAMP = 'UTC_TIME'\n",
    "    BIASDISCAQNV = 'IP.NSRCGEN:BIASDISCAQNV'\n",
    "    GASAQN = 'IP.NSRCGEN:GASAQN'\n",
    "    GASSASAQN = 'IP.NSRCGEN:GASSASAQN'\n",
    "    SOLINJ_CURRENT = 'IP.SOLINJ.ACQUISITION:CURRENT'\n",
    "    SOLCEN_CURRENT = 'IP.SOLCEN.ACQUISITION:CURRENT'\n",
    "    SOLEXT_CURRENT = 'IP.SOLEXT.ACQUISITION:CURRENT'\n",
    "    OVEN1AQNP = 'IP.NSRCGEN:OVEN1AQNP'\n",
    "    OVEN2AQNP = 'IP.NSRCGEN:OVEN2AQNP'\n",
    "    SOURCEHTAQNI = 'IP.NSRCGEN:SOURCEHTAQNI'\n",
    "    SOURCEHTAQNV = 'IP.NSRCGEN:SOURCEHTAQNV'\n",
    "    SAIREM2_FORWARDPOWER = 'IP.SAIREM2:FORWARDPOWER'\n",
    "    THOMSON_FORWARDPOWER = 'IP.NSRCGEN:RFTHOMSONAQNFWD'\n",
    "    BCT05_CURRENT = 'ITL.BCT05:CURRENT'\n",
    "    BCT25_CURRENT = 'ITF.BCT25:CURRENT'\n",
    "    BCT41_CURRENT = 'ITH.BCT41:CURRENT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select all parameters you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(parameters_raw, parameters_scaled, t1, t2):\n",
    "    print(\"Loading Data in interval {} to {}\".format(t1, t2))\n",
    "    result = {}\n",
    "\n",
    "    if parameters_raw:\n",
    "        result = ldb.get(parameters_raw, t1, t2, unixtime=True)\n",
    "\n",
    "    for k, v in parameters_scaled.items():\n",
    "        data = ldb.getScaled(k, t1, t2, scaleAlgorithm=v['scale'], scaleInterval=v['interval'], scaleSize=v['size'], unixtime=True)\n",
    "        result.update(data)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "def load_existing_data(filename, replace_column):\n",
    "    if not path.exists(filename):\n",
    "        print(\"The file {} does not yet exist, we will create a new one\".format(filename))\n",
    "        return pd.DataFrame(columns=[SourceFeatures.TIMESTAMP])\n",
    "    \n",
    "    print(\"Loading data from {}.\".format(filename))\n",
    "    if replace_column:\n",
    "        print(\"We will replace columns that already exist\")\n",
    "    else:\n",
    "        print(\"We will only append new columns\")\n",
    "        \n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def create_base_df(filename, replace_file, replace_column):\n",
    "    if replace_file:\n",
    "        df = pd.DataFrame(columns=[SourceFeatures.TIMESTAMP])\n",
    "    else:\n",
    "        df = load_existing_data(filename, replace_column)\n",
    "\n",
    "    df.set_index(SourceFeatures.TIMESTAMP, inplace = True)\n",
    "    df.index = pd.to_datetime(df.index).tz_localize('UTC')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_duplicate_times(time_series):\n",
    "    x = time_series.duplicated()\n",
    "    count = x[x].count()\n",
    "    if count > 0:\n",
    "        print(\"Time duplicates exist!\")\n",
    "\n",
    "def join_result(df, result):\n",
    "    print(\"Joining together result\")\n",
    "    for parameter, values in result.items():\n",
    "        print(\"For column {} {} datapoints exist.\".format(parameter, len(values[1])))\n",
    "\n",
    "        if parameter in df.columns:\n",
    "            print(\"Parameter {} is already in the data frame. There it has {} values. In the newly retrieved dataset it has {} values.\".format(parameter, df[parameter].count(), len(values[1])))\n",
    "            if not replace_column:\n",
    "                print(\"Skipping.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Removing old column.\")\n",
    "                df = df.drop(parameter, axis=1)\n",
    "                df = df.dropna(axis=0, how='all')\n",
    "\n",
    "        df_column = pd.DataFrame(columns=[SourceFeatures.TIMESTAMP, parameter])\n",
    "        df_column[SourceFeatures.TIMESTAMP] = pd.Series([datetime.fromtimestamp(timestamp, tz=pytz.utc) for timestamp in values[0]])\n",
    "        check_duplicate_times(df_column[SourceFeatures.TIMESTAMP])\n",
    "        df_column[parameter] = values[1]\n",
    "\n",
    "        df_column.set_index(SourceFeatures.TIMESTAMP, inplace = True)\n",
    "        df_column.dropna(inplace=True)\n",
    "\n",
    "        df = df.join(df_column, how='outer')\n",
    "\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.index = df.index.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
    "    df.index.name = SourceFeatures.TIMESTAMP\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the timeindex is duplicated, we will only keep the first occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the output to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, filename):\n",
    "    print(\"Saving result to {}\".format(filename))\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, t1, t2, parameters_raw, parameters_scaled, replace_file, replace_column):\n",
    "    result = get_result(parameters_raw, parameters_scaled, t1, t2)\n",
    "    df = create_base_df(filename, replace_file, replace_column)\n",
    "    df = join_result(df, result)\n",
    "    df = df[~df.index.duplicated(keep='first')].copy()\n",
    "    save_df(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_raw = [\n",
    "        #SourceFeatures.BIASDISCAQNV, \n",
    "        #SourceFeatures.GASAQN, \n",
    "        #SourceFeatures.OVEN1AQNP,\n",
    "        #SourceFeatures.OVEN2AQNP,\n",
    "        #SourceFeatures.SOLINJ_CURRENT,\n",
    "        #SourceFeatures.SOLCEN_CURRENT,\n",
    "        #SourceFeatures.SOLEXT_CURRENT,\n",
    "        #SourceFeatures.SOURCEHTAQNI,\n",
    "        #SourceFeatures.BCT25_CURRENT,\n",
    "        #SourceFeatures.BCT41_CURRENT,\n",
    "        SourceFeatures.SOURCEHTAQNV,\n",
    "]\n",
    "parameters_scaled = {\n",
    "        #SourceFeatures.THOMSON_FORWARDPOWER : {'scale' : 'AVG', 'interval' : 'SECOND', 'size' : '10'},\n",
    "        #SourceFeatures.BCT05_CURRENT : {'scale' : 'AVG', 'interval' : 'MINUTE', 'size' : '2'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def load_data(filename, year, month):\n",
    "    t1 = '{}-{:02d}-01 00:00:00.000'.format(year, month)\n",
    "    if month == 12:\n",
    "        month = 0\n",
    "        year += 1\n",
    "    \n",
    "    t2 = '{}-{:02d}-01 00:00:00.000'.format(year, month+1)\n",
    "\n",
    "    t1 = pytz.utc.localize(datetime.strptime(t1, '%Y-%m-%d %H:%M:%S.%f')).astimezone(tz=None)\n",
    "    t2 = pytz.utc.localize(datetime.strptime(t2, '%Y-%m-%d %H:%M:%S.%f')).astimezone(tz=None)\n",
    "    replace_file = False\n",
    "    replace_column = True\n",
    "    get_data(filename, t1, t2, parameters_raw, parameters_scaled, replace_file, replace_column)\n",
    "    print(\"Finished download of data {}/{}\\n\".format(month, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data in interval 2018-01-01 01:00:00+01:00 to 2018-02-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Jan2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 3903 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jan2018.csv\n",
      "Finished download of data 1/2018\n",
      "\n",
      "Loading Data in interval 2018-02-01 01:00:00+01:00 to 2018-03-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Feb2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 14729 datapoints exist.\n",
      "Saving result to ../Data_Raw/Feb2018.csv\n",
      "Finished download of data 2/2018\n",
      "\n",
      "Loading Data in interval 2018-03-01 01:00:00+01:00 to 2018-04-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Mar2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 103086 datapoints exist.\n",
      "Saving result to ../Data_Raw/Mar2018.csv\n",
      "Finished download of data 3/2018\n",
      "\n",
      "Loading Data in interval 2018-04-01 02:00:00+02:00 to 2018-05-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Apr2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 72572 datapoints exist.\n",
      "Saving result to ../Data_Raw/Apr2018.csv\n",
      "Finished download of data 4/2018\n",
      "\n",
      "Loading Data in interval 2018-05-01 02:00:00+02:00 to 2018-06-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/May2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 90959 datapoints exist.\n",
      "Saving result to ../Data_Raw/May2018.csv\n",
      "Finished download of data 5/2018\n",
      "\n",
      "Loading Data in interval 2018-06-01 02:00:00+02:00 to 2018-07-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Jun2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 83207 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jun2018.csv\n",
      "Finished download of data 6/2018\n",
      "\n",
      "Loading Data in interval 2018-07-01 02:00:00+02:00 to 2018-08-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Jul2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 80633 datapoints exist.\n",
      "Saving result to ../Data_Raw/Jul2018.csv\n",
      "Finished download of data 7/2018\n",
      "\n",
      "Loading Data in interval 2018-08-01 02:00:00+02:00 to 2018-09-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Aug2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 70983 datapoints exist.\n",
      "Saving result to ../Data_Raw/Aug2018.csv\n",
      "Finished download of data 8/2018\n",
      "\n",
      "Loading Data in interval 2018-09-01 02:00:00+02:00 to 2018-10-01 02:00:00+02:00\n",
      "Loading data from ../Data_Raw/Sep2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 82277 datapoints exist.\n",
      "Saving result to ../Data_Raw/Sep2018.csv\n",
      "Finished download of data 9/2018\n",
      "\n",
      "Loading Data in interval 2018-10-01 02:00:00+02:00 to 2018-11-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Oct2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 60117 datapoints exist.\n",
      "Saving result to ../Data_Raw/Oct2018.csv\n",
      "Finished download of data 10/2018\n",
      "\n",
      "Loading Data in interval 2018-11-01 01:00:00+01:00 to 2018-12-01 01:00:00+01:00\n",
      "Loading data from ../Data_Raw/Nov2018.csv.\n",
      "We will replace columns that already exist\n",
      "Joining together result\n",
      "For column IP.NSRCGEN:SOURCEHTAQNV 72353 datapoints exist.\n",
      "Saving result to ../Data_Raw/Nov2018.csv\n",
      "Finished download of data 11/2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_folder = '../Data_Raw/'\n",
    "\n",
    "year = 2018\n",
    "start_month = 'Jan'\n",
    "end_month = 'Nov'\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for m in months[months.index(start_month):months.index(end_month)+1]:\n",
    "    filename = output_folder + '{}{}.csv'.format(m, year)\n",
    "    load_data(filename, year, months.index(m)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
