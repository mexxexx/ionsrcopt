{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preliminary tests showed, that Optigrid performed well on Data of November 2018. In this notebook we will explore a few other months and look for similarities and descrepancies in the results. We will use preprocessed data, where things like source stability and voltage breakdowns are indicated. Moreover, for now we will limit ourselfs to stable running sources, i.e. time periods with a low variance and a high current in the BCT25. We use the already preprocessed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module loading\n",
    "We use the Python modules from the ionsrcopt package that will be loaded in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ionsrcopt/import_notebooks/Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ionsrcopt/import_notebooks/Clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specifiy all the columns we are interested in. There are three types: Parameters, these are the ones that will be clustered later on, Measurments and columns from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [SourceFeatures.TIMESTAMP]\n",
    "parameters = [\n",
    "        SourceFeatures.BIASDISCAQNV, \n",
    "        SourceFeatures.GASAQN, \n",
    "        SourceFeatures.SAIREM2_FORWARDPOWER,\n",
    "        SourceFeatures.SOLINJ_CURRENT,\n",
    "        SourceFeatures.SOLCEN_CURRENT,\n",
    "        SourceFeatures.SOLEXT_CURRENT,\n",
    "        SourceFeatures.SOURCEHTAQNI]\n",
    "measurements = [\n",
    "        SourceFeatures.OVEN1AQNP,\n",
    "        SourceFeatures.OVEN2AQNP,\n",
    "        SourceFeatures.BCT25_CURRENT]\n",
    "preprocessing = [\n",
    "        ProcessingFeatures.SOURCE_STABILITY, \n",
    "        ProcessingFeatures.HT_VOLTAGE_BREAKDOWN, \n",
    "        ProcessingFeatures.DATAPOINT_DURATION,\n",
    "        ProcessingFeatures.SOURCE_RUNNING]\n",
    "\n",
    "columns_to_load = time + parameters + measurements + preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the important files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../Data_Preprocessed/'\n",
    "input_files = []#['Nov2018.csv']\n",
    "input_paths = [input_folder + f for f in input_files]\n",
    "output_folder = '../Data_Clustered/'\n",
    "output_file = 'year_simulated_uniform.csv'\n",
    "output_path = output_folder + output_file\n",
    "\n",
    "cluster_logfile = output_folder + 'cluster_runs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = read_data_from_csv(input_paths, columns_to_load, None)\n",
    "df_total = fill_columns(df_total, None, fill_nan_with_zeros=True)\n",
    "df_total = convert_column_types(df_total)\n",
    "df_total.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select what data we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_values(df_total, parameters, selector):\n",
    "    data = df_total.loc[selector, parameters].values\n",
    "    weights = df_total.loc[selector, ProcessingFeatures.DATAPOINT_DURATION].values\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready we can begin clustering. But first we standard scale it, so that all parameters have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def scale_values(values, scaler):\n",
    "    if not scaler:\n",
    "        scaler = preprocessing.RobustScaler((10,90)).fit(values)\n",
    "    values_scaled = scaler.transform(values)\n",
    "    return scaler, values_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for optigrid can be chosen by visually examening the distribution of normalized data, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optigrid_params = {\n",
    "    'd' : len(parameters), \n",
    "    'q' : 1, \n",
    "    'max_cut_score' : 0.04, \n",
    "    'noise_level' : 0.05,\n",
    "    'kde_bandwidth' : [0.014, 0.011, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014],\n",
    "    'verbose' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_optigrid(values_scaled, weights, optigrid_params):\n",
    "    optigrid = Optigrid(**optigrid_params)\n",
    "    optigrid.fit(values_scaled, weights)\n",
    "    return optigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the clusters are found, we set an according column in the original dataframe containing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_df_total(df_total, optigrid, num_values, selector):\n",
    "    clusters = np.zeros(num_values)\n",
    "\n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        clusters[cluster] = i\n",
    "    df_total.loc[selector, ProcessingFeatures.CLUSTER] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we bundle all these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df_total, parameters, source_stable, optigrid_params):\n",
    "    print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "    source_stability = df_total[ProcessingFeatures.SOURCE_STABILITY] == source_stable\n",
    "    voltage_breakdown_selection = df_total[ProcessingFeatures.HT_VOLTAGE_BREAKDOWN] > 0\n",
    "    source_running = df_total[ProcessingFeatures.SOURCE_RUNNING] == True\n",
    "    \n",
    "    selector = source_stability & ~voltage_breakdown_selection & source_running\n",
    "    values, weights = select_values(df_total, parameters, selector) # First, get the data without breakdowns,\n",
    "    scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "    optigrid = run_optigrid(values_scaled, weights, optigrid_params) # and compute the clusters.\n",
    "    assign_clusters_df_total(df_total, optigrid, len(values), selector) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "    \n",
    "    print(\"Scoring voltage breakdowns\")\n",
    "    selector = source_stability & voltage_breakdown_selection & source_running\n",
    "    #values, weights = select_values(df_total, parameters, selector) # Now, get the datapoints when the voltage broke down\n",
    "    #_, values_scaled = scale_values(values, scaler) # scale it to the same ranges\n",
    "    #scored_samples = optigrid.score_samples(values_scaled) # and find the corresponding clusters.\n",
    "    #df_total.loc[selector, ProcessingFeatures.CLUSTER] = scored_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total[ProcessingFeatures.CLUSTER] = -1\n",
    "cluster(df_total, parameters, 1, optigrid_params)\n",
    "#cluster(df_total, parameters, 0, optigrid_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long term storage\n",
    "We will save the clustered data to a file.\n",
    "\n",
    "First, create the logging string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "logstring = \"[{}] \\'{}\\' cluster results saved to \\'{}\\'. Columns used: {}. Parameters used: {}\\n\".format(dt_string, input_paths, output_path, parameters, optigrid_params)\n",
    "with open(cluster_logfile, \"a\") as myfile:\n",
    "    myfile.write(logstring)\n",
    "\n",
    "logstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.astype({ProcessingFeatures.CLUSTER : 'int64'})\n",
    "df_total[df_total.shift(1)==df_total] = np.nan\n",
    "df_total.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot(df_total[SourceFeatures.SOURCEHTAQNI], df_total[SourceFeatures.BIASDISCAQNV], df_total[SourceFeatures.SOLCEN_CURRENT], linestyle='', marker='o', markersize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values_scaled, optigrid.clusters, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(a, b):\n",
    "    if a < b:\n",
    "        return 1 - a/b\n",
    "    elif a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return b/a - 1\n",
    "    \n",
    "def cluster_performance_silhouette(df_total, values_scaled, clusters, source_stability, voltage_breakdown_selection, num_clusters):\n",
    "    mean_distances = np.array([np.array([np.sum(np.linalg.norm(values_scaled[cluster]-x, axis=1)) / len(cluster) for cluster in clusters]) for x in values_scaled])\n",
    "    optigrid_cluster = df_total.loc[source_stability & voltage_breakdown_selection, 'optigrid_cluster']\n",
    "    selector = np.ones((len(values_scaled), num_clusters), dtype=bool)\n",
    "    selector[range(len(values)), optigrid_cluster] = False\n",
    "    print(mean_distances)\n",
    "    print(optigrid_cluster)\n",
    "    print(selector)\n",
    "    print(np.ma.masked_array(mean_distances, ~selector))\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'] = np.amin(np.ma.masked_array(mean_distances, selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'] = np.amin(np.ma.masked_array(mean_distances, ~selector), axis=1)\n",
    "    df_total.loc[source_stability & voltage_breakdown_selection, 'silhouette'] = np.vectorize(silhouette_coefficient)(df_total.loc[source_stability & voltage_breakdown_selection, 'mean_dist_same_cluster'], df_total.loc[source_stability & voltage_breakdown_selection, 'min_mean_dist_different_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs_euclid_squared_numpy(A, B):\n",
    "    sqrA = np.broadcast_to(np.sum(np.power(A, 2), 1).reshape(A.shape[0], 1), (A.shape[0], B.shape[0]))\n",
    "    sqrB = np.broadcast_to(np.sum(np.power(B, 2), 1).reshape(B.shape[0], 1), (B.shape[0], A.shape[0])).transpose()\n",
    "\n",
    "    return sqrA - 2*np.matmul(A, B.transpose()) + sqrB\n",
    "\n",
    "def cluster_performance_dbi(values_scaled, clusters, num_clusters):\n",
    "    print(\"values_scaled: {}\".format(values_scaled))\n",
    "    values_per_cluster = [np.take(values_scaled, c, axis=0) for c in clusters]\n",
    "    means = np.array([np.mean(c, axis=0) for c in values_per_cluster])\n",
    "    print(\"values_per_cluster: {}\".format(values_per_cluster[0][:10]))\n",
    "    print(\"means: {}\".format(means))\n",
    "    assigned_cluster_mean = np.zeros((len(values_scaled), len(values_scaled[0])))\n",
    "    for i, c in enumerate(clusters):\n",
    "        assigned_cluster_mean[c] = means[i]\n",
    "    print(\"assigned_cluster_mean: {}\".format(assigned_cluster_mean))\n",
    "        \n",
    "    dists_from_means = np.linalg.norm(values_scaled-assigned_cluster_mean, axis=1)\n",
    "    print(\"dists_from_means: {}\".format([dists_from_means[c] for c in clusters]))\n",
    "    s = np.array([np.sqrt(1./len(c) * np.sum(dists_from_means[c])) for c in clusters])\n",
    "    print(\"s: {}\".format(s))\n",
    "    \n",
    "    dists_between_clusters = all_pairs_euclid_squared_numpy(means, means)\n",
    "    np.fill_diagonal(dists_between_clusters, np.nan)\n",
    "    print(\"dists_between_clusters: {}\".format(dists_between_clusters))\n",
    "    \n",
    "    r = np.tile(s, (num_clusters, 1))\n",
    "    r = (r + r.T) / dists_between_clusters\n",
    "    print(\"r: {}\".format(r))\n",
    "    d = np.nanmax(r, axis=1)\n",
    "    dbi = np.mean(d)\n",
    "    print(\"Davies-Bouldin index per cluster: {}\".format(d))\n",
    "    print(\"Davies-Bouldin index total: {}\".format(dbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_clusters(optigrid, data, parameters):\n",
    "    values = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    result = pd.DataFrame(columns = pd.MultiIndex.from_tuples([(p, v) for p in parameters for v in values] + [('DENSITY', 'count'), ('DENSITY', 'percentage')]))\n",
    "    result.index.name = 'OPTIGRID_CLUSTER'\n",
    "    \n",
    "    for i, cluster in enumerate(optigrid.clusters):\n",
    "        cluster_data = np.take(data, cluster, axis=0)\n",
    "        mean = np.mean(cluster_data, axis=0)\n",
    "        std = np.std(cluster_data, axis=0)\n",
    "        quantiles = np.quantile(cluster_data, [0, 0.25, 0.5, 0.75, 1], axis=0)\n",
    "        cluster_description = [[mean[i], std[i], quantiles[0][i], quantiles[1][i], quantiles[2][i], quantiles[3][i], quantiles[4][i]] for i in range(len(parameters))]\n",
    "        cluster_description = [item for sublist in cluster_description for item in sublist]\n",
    "        cluster_description.append(len(cluster))\n",
    "        cluster_description.append(len(cluster)/len(data)*100)\n",
    "        result.loc[i] = cluster_description\n",
    "    return result\n",
    "\n",
    "described = describe_clusters(optigrid, data, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('DENSITY', 'percentage')]\n",
    "\n",
    "num_of_clusters_to_print = 10\n",
    "described.sort_values(by=[('DENSITY', 'percentage')], ascending=False, inplace = True)\n",
    "print(\"Sum of densities of printed clusters: {:.1f}%\".format(described.head(n=num_of_clusters_to_print)[('DENSITY', 'percentage')].sum()))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the clusters we will plot the densities of the parameters. For comparability we will use explicit ranges for the x-axis per parameter. Those ranges should be chosen beforehand by an expert to validate or falsify his intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6 # number of clusters to visualize\n",
    "data = df[parameters].values # We select the unscaled data again, because by clustering we did not change any ordering and this data corresponds to the real world\n",
    "num_datapoints = len(data)\n",
    "\n",
    "resolution = 200\n",
    "bandwidth = [1, 0.01, 1, 10, 0.1, 0.001]\n",
    "num_kde_samples = 40000\n",
    "\n",
    "parameter_ranges = [[0,0] for i in range(len(parameters))]\n",
    "parameter_ranges[0] = [-300, -200] # Biasdisc x-axis\n",
    "\n",
    "parameter_ranges[1] = [5.1, 5.3] # Gas x-axis\n",
    "#parameter_ranges[2] = [0, 3] # High voltage current x-axis\n",
    "parameter_ranges[2] = [200, 300] # SolCen current x-axis\n",
    "#parameter_ranges[3] = [900, 2100] # Forwardpower x-axis\n",
    "parameter_ranges[3] = [1200, 1300] # SolExt current x-axis\n",
    "parameter_ranges[4] = [5, 20] # Oven1 power x-axis\n",
    "parameter_ranges[5] = [0, 0.05] # BCT25 current x-axis\n",
    "\n",
    "best_clusters = sorted(optigrid.clusters, key=lambda x: len(x), reverse=True)\n",
    "for i, cluster in enumerate(best_clusters[:num_clusters]):\n",
    "    median = [described.iloc[i,described.columns.get_loc((param, '50%'))] for param in parameters]\n",
    "    plot_cluster(data, cluster, parameters, parameter_ranges, resolution=resolution, median=median, bandwidth=bandwidth, percentage_of_values=1, num_kde_samples=num_kde_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find all high voltage breakdowns that correspond to the currently considered source stability, and find out to which cluster each datapoint belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics.append(('num_of_breakdowns', ''))\n",
    "described.head(n=num_of_clusters_to_print)[wanted_statistics].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_statistics = [[(param, 'mean')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist] + [('num_of_breakdowns', '')]\n",
    "corr_described = described[wanted_statistics].corr()\n",
    "corr_described.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "wanted_statistics = [[(param, 'mean'), (param, 'std'),  (param, 'min'),  (param, 'max')] for param in parameters]\n",
    "wanted_statistics = [item for sublist in wanted_statistics for item in sublist]\n",
    "df_breakdowns.groupby('is_breakdown').describe()[wanted_statistics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "size = 10\n",
    "data = np.random.uniform(0, 1, (size, 1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(np.linalg.norm(data-x, axis=1)) for x in data]) / (size - 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "values = [0, 2, 2, 2, 3, 3, 1]\n",
    "values = np.array([[x, x] for x in values])\n",
    "clusters = [[0, 6], [1, 2, 3], [4, 5]]\n",
    "\n",
    "values, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance_dbi(values, clusters, len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_score(values, [0, 1, 1, 1, 2, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_stable = 1\n",
    "print(\"Starting clustering for source stability {}\".format(source_stable))\n",
    "source_stability = df_total['source_stable'] == source_stable\n",
    "voltage_breakdown_selection = df_total['is_breakdown'] > 0\n",
    "\n",
    "values = select_values(df_total, parameters, source_stability, ~voltage_breakdown_selection) # First, get the data without breakdowns,\n",
    "scaler, values_scaled = scale_values(values, None) # standard scale it\n",
    "print(values_scaled)\n",
    "optigrid = run_optigrid(values_scaled, optigrid_params) # and compute the clusters.\n",
    "print(values_scaled)\n",
    "#assign_clusters_df_total(df_total, optigrid, len(values), source_stability, ~voltage_breakdown_selection) # Then, assign the found clusters to the original dataframe in a new column 'optigrid_clusters'\n",
    "print(\"Calculating cluster performance cluster performance\")\n",
    "#cluster_performance_silhouette(df_total, values_scaled, optigrid.clusters, source_stability, voltage_breakdown_selection, optigrid.num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
